[{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"This semester, I finished the \u0026ldquo;senior core\u0026rdquo; of BYU\u0026rsquo;s Applied and Computational Mathematics Emphasis (ACME) program. It\u0026rsquo;s been an awesome journey, and since other people\u0026rsquo;s thoughts on the internet were influential in leading me to join the program, I thought I\u0026rsquo;d share mine.\nHow I ended up in ACME #I started my journey at BYU studying electrical engineering. My second semester, I took Digital Systems (EC EN 220) and Circuit Analysis (EC EN 240). At the close of the semester, as I prepared for my final exams in a windowless room on the fourth floor of the Clyde (BYU\u0026rsquo;s old engineering building), my thoughts wandered. I really enjoyed what I was learning, but the physics class I was also taking (PHSCS 123) and conversations with my astronomy major then-girlfriend (now wife) made me wish I could explore the fundamentals more. I wanted more theory, feeling that if I thoroughly understood the theory behind what I was doing, I could go farther in the long term. I wanted to learn to learn, afraid I was studying techniques and tools that wouldn\u0026rsquo;t be relevant in the long term.\nNow, I don\u0026rsquo;t think all of my thoughts were spot on. That was four years ago (early 2021), and even with the advent of ChatGPT and other LLMs, I don\u0026rsquo;t think the electrical engineering skills I was learning will be irrelevant anytime soon, if ever. The thoughts I was having led me down a good path, though. I wanted to find a way to bridge the gap between the exciting hands-on applications of electrical engineering with the rigor and theory in a program like physics. I wanted to have the deep theoretical understanding necessary to adapt to the jobs of the future, while still having useful skills today, even if some tools I learn might go obsolete.\nThere in the Clyde, my mind drifted. I remembered I\u0026rsquo;d seen a stranger a few weeks earlier wearing an ACME shirt while playing pool with my brother, and decided to look the program up again. As I read about ACME on its website as well as various forums, it felt right, it felt exciting! It dawned on me that such a program might fulfill my goal of bridging intense theory with application, and that mathematics is and always will be relevant in any technological field.\nMy experience in the program #Soon afterwards, I left my studies to serve a two-year mission for the Church of Jesus Christ of Latter-day Saints. Upon my return, ACME still felt like a good path, so I stuck with it. In my year studying electrical engineering, I\u0026rsquo;d taken all of the prerequisite math classes like linear algebra and multivariable calculus, so I just needed to take an introductory proofs class (MATH 290) and a bit of real analysis (MATH 341) to be eligible to start the \u0026ldquo;junior core\u0026rdquo; of ACME (which began every fall at the time). I took them over the summer and jumped into the junior core that fall (my sophomore year), hoping that having the advanced math done earlier would lead to cool opportunities down the line.\nThe heart of the ACME program consists of the \u0026ldquo;junior core\u0026rdquo; and \u0026ldquo;senior core\u0026rdquo;, which each last two semesters. Each semester, students take an envelope of four interdependent classes. Two are traditional lecture classes, and two are programming lab classes with no lecture component. Most students take them all consecutively for two years, so the people you start the core with (your \u0026ldquo;cohort\u0026rdquo;) are the same people you finish with. We were encouraged to take advantage of the junior and senior study roomsâ€”two large rooms full of whiteboards and long tables, reserved for ACME juniors and seniors to collaborate on homework and projects.\nIn my opinion, students who didn\u0026rsquo;t take advantage of the study rooms missed out on what may be the most valuable part of the ACME program: the people you meet. I met some stupendous people in the junior and senior rooms. Last week we had our final ACME gathering, a presentation session to showcase our final data modeling projects this past semester. I realized I knew the names and personalities of just about everyone in the program\u0026hellip; except for a few who didn\u0026rsquo;t make a habit of collaborating with their peers in the ACME rooms. They missed out on a once-in-a-lifetime opportunity! My peers became my team, and some, my closest friends.\nThe program is by no means perfect. I\u0026rsquo;ll list what I think are its best and worst aspects.\nWhat makes ACME great # It fulfilled my expectationsâ€”a thorough treatment of mathematical theory balanced with plenty of fascinating real-world applications. I learned to learn, just as I\u0026rsquo;d hoped. (Admittedly, this probably happens in any intense university program.) Super cool curriculum developed by BYU faculty, supposedly unlike most applied math programs. See information about textbooks and complete lab manuals and textbooks\u0026rsquo; tables of contents. Faculty is very invested in the program, they\u0026rsquo;re passionate about it. It\u0026rsquo;s only twelve years old. ACME won the American Mathematical Society\u0026rsquo;s 2024 \u0026ldquo;Award for an Exemplary Program or Achievement in a Mathematics Department\u0026rdquo;. Can you spot me in the picture? Faculty does a great job facilitating and encouraging collaboration among students. The senior core has lots of big group projects. Since I\u0026rsquo;d spent the junior core collaborating on homework in the junior room, I knew there were plenty of great people I could choose to work with on them. The lab classes augment the lecture classes very well. The labs are all in Python (supposedly they were originally going to be in MATLAB! Glad that didn\u0026rsquo;t happen). They introduce you to a broad array of useful Python packages (particularly NumPy, SciPy, and scikit-learn) as well as object-oriented programming. I found that the labs prepared me well for my current research job, which in turn helped me land an internship. Heavy focus on employability. The math department hosts networking events regularly and practically begs you to let them help you with your resume and such Applications well beyond pure mathematics. See list of ACME concentrations. Great food haha! These folks know how it\u0026rsquo;s done. Abundant Cafe Rio and Panda Express at a ton of events. What it\u0026rsquo;s still working out # \u0026ldquo;Volume 4\u0026rdquo;, the program\u0026rsquo;s textbook on dynamic systems, control theory, and related topics (see its outline), still needs a lot of work. My peers and I frequently felt frustrated because Volume 4 is still pretty rough around the edges. Lots of typos in the textbook draft and not-so-clear explanations. The homework questions in Volume 4 also didn\u0026rsquo;t seem to probe deep understanding of the material, so I\u0026rsquo;m still not sure I understand it as well as I\u0026rsquo;d like. I should note that Volumes 1 and 2 (the textbooks for the junior core) are superb, and have been published (Volume 1, Volume 2), but the faculty is still working on volumes 3 and 4 (see textbooks\u0026rsquo; tables of contents). Volume 3 is in great shape, I\u0026rsquo;d say. Volume 4 needs some work. The workload can be very intense. The pressure just about breaks everyone at some point in their experience. In the junior core, each homework assignment (students have six homework sets a week) took me around 3 hours, even 4 on bad days. Add on top of that the 2 or 3 hour labs assigned twice a week, and you end up pretty busy, since you\u0026rsquo;re also expected to do textbook readings before each class. (Not to mention the mental strainâ€”breaks are a must!) In the senior core, each homework set took around 2 hours, but there are also two end-of-semester group projects assigned that each took me between 20 to 30 hours. The core classes technically total 8 credit hours per semester (12 credits is the minimum one has to take to keep scholarships), but took a lot more time than any other 8 credits would have. I\u0026rsquo;ve started telling people to schedule their semester thinking of ACME as 12 credits rather than 8, since it will take over your life for two years, in a sense. I sometimes wish the number of credit hours reflected the workload, although I\u0026rsquo;m aware of some reasons that such an increase in credit hours is not possible. Prideful attitudes. It seems like this is getting less common in the program (I hope it is at least). I noticed that students in other programs, like engineering, physics, and APEX Math (BYU\u0026rsquo;s more mainstream math program) sometimes harbored some resentment for the ACME program because of prideful people they\u0026rsquo;d interacted with. I remember talking to an engineering student about ACMEâ€”he told me that he knew about the program, since a peer in one of his engineering classes was in ACME, and constantly reminded everyone in the class how easy the integrals and other tricky math concepts were for him! Nobody wants to work with a person like that, and ACME\u0026rsquo;s grandiose promises (\u0026ldquo;world domination\u0026rdquo; comes to mind ðŸ˜‰) might contribute to some students having such attitudes. How to succeed in ACME #Based on my own experience and what I saw in my peers, I think these are an ACME student\u0026rsquo;s tickets to success:\nSet boundaries. My wife helped me set boundaries to make sure I was taking care of myself, which worked wonderfully, but not everyone fared so well. To their credit, the faculty repeatedly encouraged everyone to put their health and well-being first, even when that meant not finishing homework. In practice, it\u0026rsquo;s hard to resist staying up and instead turn in your chicken-scratch half-done hand-wavy homework when it\u0026rsquo;s only 9 pm, but I learned to embrace it. I still did fine on exams. Internalize the law of diminishing returns. I found that I could put in 70% of the time and learn 95% of the material on homework assigments. Would that last 5% have been worth it? I don\u0026rsquo;t think so. I got A\u0026rsquo;s in all my ACME classes except for a semester of Volume 4 because I didn\u0026rsquo;t do so well on the final and ended up with an A minus. I learned a ton in this program. Don\u0026rsquo;t compare yourself to othersâ€”celebrate others! All the people who make it through ACME are stellar students. Growing up, I found that I was quick at math. I got used to being at the top of my class, and I aced math and physics exams with little real effort. In ACME, for the first time, I found myself in a community where I was overwhelmingly mediocre. I saw dozens of people all around me who seemed to understand things much faster than I could, and it was easy to want to compare myself to them. Throughout the program, I often reflected on the third chapter of Malcolm Gladwell\u0026rsquo;s Outliers. In it, Gladwell proposes that metrics for intellectual ability like test scores and IQ are only meaningful up to a threshold. Beyond a certain point, once someone is \u0026ldquo;smart enough\u0026rdquo;, having an even higher score doesnâ€™t make much of a differenceâ€”they are just as likely to succeed as those with the highest scores. While some people in ACME really stood out as bona fide geniuses, to me, everyone seemed to easily surpass the intelligence \u0026ldquo;threshold\u0026rdquo;. There is no need to compare in ACME because if you\u0026rsquo;ve made it to the junior core, you are smart enough, and other attributes like emotional and interpersonal skills will probably have a much greater impact on your success than your raw intelligence. I found that most people in the program love helping each other. When my friends succeed, I succeed too, because I know that they want to help me, and I want to help them! In my cohort there was a culture of helping peers understand tricky homework problems, even if it took time. Eventually I sought to give as much as I received. Many concepts were too hard for me to grasp on my own, and my peers taught me. Then when I saw others struggling, I\u0026rsquo;d pass the understanding on to them the best I could. Everyone benefits in such a culture, and I hope other cohorts experienced something similar. It is easy to celebrate others\u0026rsquo; successes, since their success is yours too! Conclusion #The ACME program is a great idea executed well. I had no idea I was able to learn so much so quickly. The material in the core curriculum is foundational, and will remain relevant in most technological fields well past my lifetime. The faculty members involved are fully invested in the program, so ACME will keep improving just as it has throughout the twelve years of its existence. I\u0026rsquo;m glad I chose to be a part of it!\n","date":"28 April 2025","permalink":"/posts/byu-acme-thoughts/","section":"Posts","summary":"","title":"Thoughts on BYU's ACME math program upon finishing core classes"},{"content":" A few months back I dove into a book on making sense of complex data with simple models as a part of my research (High-Dimensional Data Analysis with Low-Dimensional Models, by Wright and Ma). I learned about and implemented a fun demo of Principal Component Pursuit (PCP). My advisor loved it! We played with feeding the algorithm different kinds of simulated data, and wondered about how we might apply such an idea to better understand 3D tomograms.\nRecently my advisor was out of town. He\u0026rsquo;s currently introducing a new class on scientific computing (Computational Physics, PHSCS 530) here at BYU. I was surprised when he asked me to fill in for him, teaching about my Principal Component Pursuit project!\nIn preparation, I cleaned up my code a little, made a new demonstration, and prepared my lecture. It went really well, and taught me so much.\nHere I\u0026rsquo;ve essentially written out what I taught the class, minus a few side tangents about machine learning and dimensionality reduction from questions that students asked.\nRead on to learn about it!\nA lesson on Principal Component Pursuit #Presentation Slides (Google Slides)\nA signal \\(\\bf{Y}\\) can often be broken up into two partsâ€”a part that stays mostly the same over time (\\(\\bf{L}\\)), and part that is usually zero (\\(\\bf{S}\\)).\nTo formalize what this means, we say that \\(\\bf{L}\\) should be low rank, and \\(\\bf{S}\\) should be sparse.\nThe plot in the slide provides some intuition for what a sparse signal is. You can see that \\(\\bf{S}\\) is usually zero (it is sparse), and the original signal \\(\\bf{Y}\\) is the sum of the two components \\(\\bf{L}\\) and \\(\\bf{S}\\).\nThe plot can\u0026rsquo;t depict a low-rank signal well since this signal \\(\\bf{Y}\\) is only one-dimensional, but a low-rank signal in this context may be understood as a signal that stays mostly the same, most of the time. In a moment we will look at a higher dimensional signal and talk more about this.\nYour browser does not support the video tag. A video is a high dimensional signal! Just as the previous plot showed a signal decomposed into a low-rank and a sparse component, here is a short video of me in my lab decomposed in a similar way, to motivate what I\u0026rsquo;m about to share with you.\nHere is a three-dimensional signal. Each column represents the signal at a point in time.\nThe rank of a matrix is the number of linearly independent columns (or rows) in the matrix. Here \\(\\bf{Y}\\) is full-rank, meaning that the rank is as high as possible for a matrix of this size (\\(\\text{rank}(\\bold{Y}) = 3\\)). (Given an \\(m \\times n\\) matrix, the highest the rank can be is the smallest dimension, either the number of rows \\(m\\) or the number of columns \\(n\\).)\nNow we will decompose \\(\\bold{Y}\\) into a low-rank and a sparse component such that \\(\\bold{Y} = \\bold{L} + \\bold{S}\\). What do you think this will look like?\nHere is one good decomposition. The rank of \\(\\bold{L}\\) is as low as possibleâ€”just 1! Meanwhile, \\(\\bold{S}\\) is somewhat sparse, with only 5 nonzero elements. And, as desired, \\(\\bold{Y} = \\bold{L} + \\bold{S}\\).\nHow might we formalize and automate such a decomposition? We want the rank of \\(\\bold{L}\\) and the number of non-zero elements of \\(\\bold{S}\\) to both be low. So let\u0026rsquo;s minimize that, with a parameter \\(\\lambda \u0026gt; 0\\) to control the relative importance of each.\nAnd to make sure it\u0026rsquo;s a true decomposition of \\(\\bold{Y}\\), we add the condition that \\(\\bold{Y} = \\bold{L} + \\bold{S}\\).\nThis problem perfectly describes our goal mathematically, but it is hard to program a routine that does this automatically. It isn\u0026rsquo;t a convex problem, so we cannot expect it to have unique optimizers (in other words, we cannot expect there to be a unique \\(\\bold{L}\\) and \\(\\bold{S}\\) that minimize \\(\\text{rank}(\\bold{L}) + \\lambda \\Vert \\bold{S} \\Vert_0\\)). Solving this problem automatically would be very hard.\nSo we set up a similar problem that is easy to solve. We want to make the problem convex, a mathematical condition that implies that any local minimum of the problem is also a global minimumâ€”generally, this means that there is a unique optimizer for the problem, which will make it easy to solve.\nWe replace \\(\\text{rank}(\\bold{L})\\) with the nuclear norm of \\(\\bold{L}\\), denoted \\(\\Vert \\bold{L} \\Vert_*\\). This represents the sum of the singular values of \\(\\bold{L}\\), which are closely tied to its eigenvalues. In particular, if \\(\\bold{L}\\) has a low rank, then most of \\(\\bold{L}\\)\u0026rsquo;s singular values are zero. (Specifically, \\(\\text{rank}(\\bold{L}) = \\Vert \\boldsymbol{\\sigma}(\\bold{L}) \\Vert_0\\) where \\(\\boldsymbol{\\sigma}(\\bold{L})\\) yields the singular values of \\(\\bold{L}\\)). This norm is convex.\nWe replace \\(\\Vert \\bold{S} \\Vert_0\\) with the largest column sum (taking the absolute value of each element) of \\(\\bold{S}\\), denoted \\(\\Vert \\bold{S} \\Vert_1\\). If most of the elements of \\(\\bold{S}\\) are 0, then we may safely assume that the largest column sum of \\(\\bold{S}\\) will be small, barring the possibility that the non-zero elements of \\(\\bold{S}\\) are huge in magnitude. This norm is also convex.\nThe new problem we have constructed is convex, and thus, any \\(\\bold{L}\\) and \\(\\bold{S}\\) that minimize the problem locally also minimize it globally! This makes our search much easier. We call this new problem Principal Component Pursuit.\nWright and Ma provided pseudocode in their book for ADMM (Alternating Direction Method of Multipliers), an algorithm for solving convex optimization problems that happens to work well for this problem. I implemented it in Julia.\n(I could have used a standard Julia library like Convex.jl, which would probably be more robust, but decided to implement the pseudocode to practice.)\nNow let\u0026rsquo;s talk about how a video fits into this whole framework, like the earlier video of me walking in my lab.\nA video is a signal. For simplicity\u0026rsquo;s sake, I\u0026rsquo;m working with my videos in grayscale, but these methods should also apply (with some modifications) in color. Here I have an example \u0026ldquo;video\u0026rdquo; with three frames, each of which is just 4 by 6 pixels.\nEach frame is a \u0026ldquo;block\u0026rdquo; of pixel intensity values. We can \u0026ldquo;flatten\u0026rdquo; each frame into a vector of values.\nApplying this \u0026ldquo;flattening\u0026rdquo; to each frame of the video, we can convert the video (which is a 4Ã—6Ã—3 array) into a 24Ã—3 matrix, where each column represents the signal at a point in time (a frame).\nNow we\u0026rsquo;re ready to throw the video into the optimization problem previously described! I\u0026rsquo;ll set \\(\\lambda = 1/\\sqrt{\\max(m, n)}\\), where \\(\\bold{Y}\\) is \\(m \\times n\\), a rule-of-thumb suggestion by Wright and Ma in their book.\nYour browser does not support the video tag. Here\u0026rsquo;s the video I showed earlier again. Each frame has 111,000 pixels, and there are 195 frames in the video, so \\(\\bold{Y}\\) is a 111,000Ã—195 matrix. You can see that the rank of the original signal (the original grayscale video) is maximal, at 195.\nAfter Principal Component Pursuit, the low-rank component has a rank of just 4. You can see that it only has a few unique framesâ€”one where I\u0026rsquo;m standing with my hand seemingly chopped off (it was moving, so the algorithm added it to the sparse component), another where I\u0026rsquo;m not there at all, and another where I\u0026rsquo;m just visible by the fume hood.\nThe sparse component shows what was missing in the low-rank component, particularly, any part of the video that was moving. Only 2.2% of the pixels in the video are non-zero.\nIt looks like the new problem we constructed,\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\Vert\\bf{L}\\Vert_* + \\lambda \\Vert\\bf{S}\\Vert_1 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S}, \\end{aligned} $$\nis a good replacement for the original problem\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\text{rank}(\\bf{L}) + \\lambda \\Vert\\bf{S}\\Vert_0 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S}! \\end{aligned} $$\nYour browser does not support the video tag. Here\u0026rsquo;s another demo.\nThe original video had 723 frames, and as before, 111,000 pixels per frame. Thus \\(\\bold{Y}\\) is a 111,000Ã—723 matrix, with yet again full rank (723).\nThe low rank component isn\u0026rsquo;t as low rank as before, at 44. Perhaps a different choice of \\(\\lambda\\) would help here. Notice the digits displayed on the watch. It seems like 9 is Principal Component Pursuit\u0026rsquo;s favorite number!\nAbout 12.8% of the pixels in the sparse component are non-zero.\nMaybe this video is a little harder to decompose than the other one!\nOne simple application of this technique is to detect and quantify motion in a video. In this demo, I\u0026rsquo;ve counted the number of non-zero elements in each frame of the sparse component of the video and plotted it over time to measure motion.\n(Here\u0026rsquo;s some bonus contentâ€”the same motion demo on the original video of me walking across the lab!) There are a number of issues with this approach.\nIt doesn\u0026rsquo;t take advantage of spatial and temporal relationships in video.\nWe converted the video to a matrix \\(\\bold{Y}\\), where each column is a frame of the video, and each row is a single pixel\u0026rsquo;s intensity over time. Since all we\u0026rsquo;re concerning ourselves with is the rank and sparsity of the components of \\(\\bold{Y}\\), we could mix all the rows (frames) around and it wouldn\u0026rsquo;t make a difference (although it certainly should, especially in the context of the motion demo I did)! Likewise mixing the columns (pixels) around. No change. It scales quite poorly with the length of the video.\nThe walking-across-the-lab video took about 20 seconds on my ThinkPad to process, while the fridge video took a couple of minutes, even though the fridge video is only 3 or 4 times longer. Setting \\(\\lambda\\) is difficult. A better choice of \\(\\lambda\\) might\u0026rsquo;ve helped in the fridge video demo.\nReferences\nWright and Ma\u0026rsquo;s book The original demo and description I made The project\u0026rsquo;s GitHub ","date":"25 February 2025","permalink":"/posts/teaching-pcp/","section":"Posts","summary":"","title":"Teaching Principal Component Pursuit in a computational physics class"},{"content":"This project is a paper with associated code.\nAs a student in BYU\u0026rsquo;s Applied and Computational Mathematics Emphasis (ACME), this semester I took a class in modeling with data and uncertainty. In it my peers and I learned (among many other things) about the theoretical underpinnings of various classification and regression techniques. To enhance our practical understanding of machine learning, we were assigned an open-ended group projectâ€”given a list of a dozen or so datasets from Kaggle, we were pick a dataset, ask some interesting question about the data, and attempt to answer it with the techniques we\u0026rsquo;d learned this semester. Then we were to summarize our findings in a five-page paper.\nI wrote the paper with my classmate Rebecca Gee. She focused primarily on data preparation and understanding what the features in the dataset actually meant (what in the world is \u0026ldquo;Income Composition of Resources\u0026rdquo;? She figured it out), and I focused on the feature engineering and machine learning methods.\nI usually manage my code with Git and GitHub, but this time I opted for a Jupyter notebook in Google Colab to avoid training the models on my system. View the code here. It proved challenging to export the large notebook to other formats (.pdf, .html). Troubleshooting would\u0026rsquo;ve been easier had I coded in a more robust way, by saving trained models and such. The time I spent wrestling with the notebook export reminded me to be more careful next time, in spite of any looming deadlines.\nThe original paper is a .pdf document, but for a better reading experience, I\u0026rsquo;ll copy it as best I can below.\nPredicting Future Life Expectancy with Present Data #Matthew Ward, Rebecca Gee #\nAbstract. Predicting life expectancy is crucial for understanding global health trends and guiding policy decisions. We aim to predict the life expectancy of a country five years into the future using socio-economic and health-related features. After engineering features to normalize trends and focus on deviations from global averages, we employ several machine learning models, including Random Forest and Gradient Boosting, to make predictions. Initial results demonstrate modest predictive performance when evaluated on unseen countries, with features like income composition and vaccination rates contributing significantly. However, cluster-based evaluation reveals that the models struggle to generalize across diverse regions, highlighting challenges in capturing global heterogeneity.\n1. Research question and overview of the data #Using the WHO Life Expectancy dataset, how well can we predict the life expectancy of a given country in 5 years from now?\nPrevious research done with similar datasets used a variety of techniques. Lipesa, et al. used extreme gradient boosting (XGBoost) and pointed to thinness, schooling, infant deaths and BMI as leading factors in Life Expectancy. Gill et al. used linear regression to find a correlation between adult mortality and GDP per capita.\nThe Life Expectancy Dataset contains data from countries over a fifteen year period. Below we outline a few.\nFeature Explanation Life Expectancy Average national life expectancy (years) Alcohol Alcohol consumed per capita (liters) Percentage expenditure Government expenditure on health (% of GDP) Polio Percent population vaccinated against polio 15 other features. See Appendix A. This data captures a variety of different ideas useful in predicting life expectancy, for instance, immunizations, BMI and adult mortality seem to be good indicators of life expectancy. Other datasets with less information or less countries would be less suited to this model.\nOne weakness of the dataset is an abundance of missing, incomplete or bad data, which we can correct or impute. The columns associated with diseases often use different metrics, and are therefore hard to compare. For example, it would be hard to ask which disease has the biggest impact on life expectancy, since each disease is not measured the same way.\nWe will use this dataset to predict life expectancy in the future, and to determine what components most contribute to life expectancy.\n2. Data Cleaning / Feature Engineering #There were four countries (North and South Korea, Sudan, and South Sudan) that lacked a substantial portion of their data, necessitating their removal. We also dropped the Population column because it was missing data and was not a key feature of the model.\nWe also noticed that there were problems with the ``GDP\u0026quot; data column. Much of it was missing or inaccurate. For this reason, we chose to take the GDP from another dataset, which we verified had accurate and complete numbers by comparing a random subset of the numbers to those found by the World Bank.\nWe imputed the rest of the missing data using an imputer built with the k-Nearest Neighbors algorithm (KNNImputer from scikit-learn).\nWe hope that given statistics for a year of data, we can predict life expectancy five years later. Now, to do this, we will not simply train a model to predict life expectancy, since it is increasing worldwide.\nFigure 1. Average life expectancy is increasing worldwide. Thus we create new features that will more accurately indicate if the model is learning to predict meaningful trends.\nDeviation from worldwide mean. Unlike life expectancy worldwide, this feature will on average be flat. Deviation from worldwide mean in five years. Of course, it is not possible to calculate this feature for the last five years represented in the dataset. After calculating this feature wherever it is possible, we drop rows in the last five years. Five-year change in deviation from worldwide mean. This is the output of our model. It should be relatively flat and close to zero, unlike a five-year change in life expectancy, which is normally greater than zero. 3. Data Visualization and Basic Analysis # Figure 2. Life expectancy is not always stable. We have already seen that on average, life expectancy is increasing, however, it is not always stable. In some countries it wildly fluctuates, and in others it is quite continuous (Figure 2). Peru is an example of a country with a more or less constant rate of change in its life expectancy, matching the trend of the worldwide mean quite nicely. However, we see that in Spain, there was a sharp increase for about two years, after which it resumed it\u0026rsquo;s previous value. Life expectancy in Iraq varies wildly around the mean and Rwanda increases steadily with a couple irregularities where life expectancy was much higher than one might expect.\nLife expectancy prediction is not a trivial problem. The data often contradicts common assumptions about what helps and hurts, possibly because of conflating variables (see Figure 3).\nFigure 3. Prediction of life expectancy is not trivial. 4. Learning Algorithms and In-depth Analysis #We split the dataset into training (80%) and testing (20%) sets, grouping by country.\nWe trained models on the deviation from the worldwide mean in five years, a feature we engineered as described in Section 2.\nUsing Bayesian optimization via Optuna, we optimized hyperparameters for Random Forest, Gradient Boosted, XGBoost, as well as Ridge regression models with cross-validation, again grouping data by country. As can be seen in our code, after hyperparameter tuning, the XGBoost regressor had the best average validation score during cross-validation. On the test set it had a coefficient of correlation of 0.30, indicating that while its performance was not stellar, it did learn something meaningfulâ€”after all, it was scored on data from countries it had never seen before.\nFigure 4. The model is able to make some meaningful predictions (top), but sometimes fails (bottom). Since the model uses data from a given year to predict the life expectancy five years from then, the predicted line begins in 2005 and continues to 2020. The outputs of the model give the change in life expectancy over 5 years, so adding those to the current life expectancy gives us the orange line as shown. The five most important features for the best model were (in order) present life expectancy (naturally), HIV/AIDS cases, Polio vaccinations, ICOR, and Diphtheria vaccinations (see Appendix A).\nHowever, changing the way we split the data for training and testing proves that our initial model is not generalizable. In our new testing set, we clustered the data into eight clusters by performing PCA on the normalized data, using k-means clustering to refine the groups, and merging clusters that were too small.\nFigure 5. The clusters visualized using the first two principal components. See Appendix B. We used these clusters as folds in cross-validation, except for Cluster 4, which we randomly selected and set apart as a test set. This means that the models saw plenty of data, but had to use what they learned on countries that were substantially different than anything they had previously seen.\nAs before, we trained various models on the rest of the data, tuning hyperparameters for various models with cross-validation to optimize performance. Using the best model (XGBoost), we obtained a coefficient of correlation of 0.15 on the test set. Compared to the previous coefficient of correlation of 0.30, this is drastically worse.\nIt is possible to predict future life expectancy in a target country given data from countries similar to it. However, predicting life expectancy in a country that is different from the rest is much more difficult.\n5. Ethical Implications and Conclusions #Predicting life expectancy in the future has several important ethical implications. The potential for future humanitarian work and prevention of decreasing life expectancy is powerful. The model is quite harmless, but there are a few ways it could be used negatively.\nFor instance, in war, if an enemy had enough data, they could identify key statistics to weaken the other country\u0026rsquo;s health using methods like these. In addition, overreacting to the model could create a self-fulfilling prophecy.\nTo ensure the model functions effectively, it requires data from a diverse range of countries. Organizations like the WHO must ensure this data is managed responsibly and securely.\nIn conclusion, predicting life expectancy in the future is possible with sufficient data. In order to predict the life expectancy, a model with many different countries is needed, and the results can be relatively accurate on a short time period.\nAppendix #A. Features of Dataset #(in no particular order)\nStatus (Developed/Developing) Life Expectancy (age) Adult Mortality (Probability of dying between 15-60 years of age per 1000) Infant deaths (number per 1000) Alcohol (consumed per capita in liters) Percentage expenditure (Expenditure on health percentage of GDP) Hepatitis B (Percentage immunizations) Measles (number of cases per 1000) BMI (average of population) Under 5 deaths (number per 1000) Polio (Percentage immunizations) Total expenditure (percentage expenditure on health of total government expenditure) Diphtheria (Percentage immunizations) HIV/AIDS (Deaths per 1000 live births) GDP (per capita in USD), Population (of country) Thinness 1-19 years (prevalence of thinness) Thinness 5-9 years (prevalence of thinness) Income Composition of Resources (ICOR) (Human Development Index in terms of ICOR) Schooling (number of years of schooling) B. Countries in Clusters #Cluster 1:\nAfghanistan, Angola, Central African Republic, Chad, China, Congo, Democratic Republic of the Congo, Equatorial Guinea, Ethiopia, Gabon, Guinea, Haiti, India, Lao People\u0026rsquo;s Democratic Republic, Liberia, Niger, Nigeria, Somalia, Uganda\nCluster 2:\nAlbania, Antigua and Barbuda, Armenia, Bahrain, Belize, Brunei Darussalam, Cabo Verde, Colombia, Cuba, Egypt, El Salvador, Fiji, Grenada, Guatemala, Guyana, Honduras, Iran (Islamic Republic of), Israel, Jamaica, Jordan, Kuwait, Kyrgyzstan, Libya, Malaysia, Maldives, Mauritius, Mexico, Mongolia, Morocco, Nicaragua, Oman, Panama, Paraguay, Peru, Qatar, Saint Vincent and the Grenadines, Sao Tome and Principe, Saudi Arabia, Seychelles, Singapore, Sri Lanka, Tajikistan, Thailand, The former Yugoslav republic of Macedonia, Tunisia, Turkmenistan, United Arab Emirates, Uzbekistan, Viet Nam\nCluster 3:\nAlgeria, Azerbaijan, Bolivia (Plurinational State of), Bosnia and Herzegovina, Costa Rica, Dominican Republic, Ecuador, Georgia, Iraq, Kiribati, Lebanon, Micronesia (Federated States of), Montenegro, Philippines, Samoa, Solomon Islands, Suriname, Syrian Arab Republic, Tonga, Trinidad and Tobago, Turkey, Ukraine, Vanuatu, Venezuela (Bolivarian Republic of)\nCluster 4:\nArgentina, Bahamas, Barbados, Belarus, Brazil, Bulgaria, Chile, Croatia, Cyprus, Czechia, Estonia, Finland, Greece, Hungary, Italy, Kazakhstan, Latvia, Lithuania, Malta, Poland, Portugal, Republic of Moldova, Romania, Russian Federation, Saint Lucia, Serbia, Slovakia, Slovenia, Spain, United Kingdom of Great Britain and Northern Ireland, United States of America, Uruguay\nCluster 5:\nAustralia, Austria, Belgium, Canada, Denmark, France, Germany, Iceland, Ireland, Japan, Luxembourg, Netherlands, New Zealand, Norway, Sweden, Switzerland\nCluster 6:\nBangladesh, Benin, Bhutan, Botswana, Burkina Faso, Burundi, Cambodia, Cameroon, Comoros, CÃ´te d\u0026rsquo;Ivoire, Djibouti, Eritrea, Gambia, Ghana, Guinea-Bissau, Indonesia, Kenya, Lesotho, Madagascar, Malawi, Mali, Mauritania, Mozambique, Myanmar, Namibia, Nepal, Pakistan, Papua New Guinea, Rwanda, Senegal, Sierra Leone, South Africa, Swaziland, Timor-Leste, Togo, United Republic of Tanzania, Yemen, Zambia, Zimbabwe\n","date":"17 December 2024","permalink":"/projects/acme-volume-3-project/","section":"Projects","summary":"","title":"Predicting future life expectancy in countries using present data"},{"content":"","date":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":" As a student in applied mathematics, I spend much of my time writing code, taking lecture notes, and working out proofs, all at breakneck speed. Many of my peers use \\(\\LaTeX\\) to take notes and do homework. While I appreciate \\(\\LaTeX\\) and often use it for formal write-ups, I don\u0026rsquo;t love not being able to quickly draw diagrams, change colors, make up new notation on the fly, etc. Handwriting is infinitely flexible, and in my experience, much quicker when I\u0026rsquo;m trying to pump out my homework and move on.\nA notebook isn\u0026rsquo;t enough. I want my notes to be stored in the cloud. I want them to be organized and easily accessible. I want them to be stored in .pdf format so they\u0026rsquo;re future-proof. And I don\u0026rsquo;t want to buy a tablet. Why? Well, good tablets are expensive, I like my laptop, I don\u0026rsquo;t want to bring two devices to school, and I like being able to seamlessly copy snippets of code I\u0026rsquo;m writing and textbooks I\u0026rsquo;m referencing into my notes, among other things.\nAbout two years ago, the solution came to me. I had the idea when I was a missionary in Cali, Colombia, walking down the street just a few months before it was time for me to go home. I was an office secretary and I had just used the mission credit card to pay at a notary, and signed for it using a screenless credit card signature pad attached to the card reader.\nI thought as I walked, \u0026ldquo;why don\u0026rsquo;t they just take one of those and scale it up?\u0026rdquo; I was certain that such a product must exist.\nSure enough, it doesâ€”but it isn\u0026rsquo;t generally marketed to people like me. Many companies, the foremost of which is Wacom, manufacture screenless tablets with pressure-sensitive pens to do digital art and animation. The level of quality varies greatly. The fanciest one I\u0026rsquo;ve used has a screen and sells for $3,500 USD (The research group I currently work with has one to annotate tomogramsâ€”it\u0026rsquo;s a Wacom Cintiq Pro 27).\nI don\u0026rsquo;t have that kind of cash. So I saved a few hundred bucks and got one of these on Amazon for $50.\nThere\u0026rsquo;s really nothing special about this specific model (Huion H610PRO v2). It\u0026rsquo;s just a generic off-brand screenless Wacom tablet that connects to your computer through a USB cable. People usually use these to do digital art and animation. I have found that they work astoundingly well for notetaking as well. Neither the pen nor the tablet use batteries, so when I\u0026rsquo;m on the go, all I have to charge is my laptop.\nI\u0026rsquo;ve been using my tablet for a year and a half now, and it\u0026rsquo;s still working like new. I only had to replace the pen nib once because I was fiddling with it and lost it on the floor. Whoops!\nThe trickiest part about all this is how to set it up. Naturally with unstandardized hardware like a screenless tablet there are lots of driver issues and subtleties to keep in mind. In addition, I use Linux (Debian 12 with GNOME and KDE), so things are often a little less intuitive (but more fun). Huion (the manufacturer) makes a good driver, and I also like using OpenTabletDriver, an open-source alternative. Things get a little dicey with a multi-monitor setup but I think I\u0026rsquo;ve got it figured out now.\nI take notes in Xournal++. I love it. It\u0026rsquo;s everything I needed in notetaking software and more. My tablet has eight buttons on the left, which I\u0026rsquo;ve mapped to the following in Xournal++: undo, delete, pencil, zoom in, zoom out, new page, the lasso selection tool, and the hand tool. I also have an on-screen icon in my GNOME panel to take screenshots (Screenshot Tool), which is especially useful when I want to paste snippets from textbooks (all of which I access digitally) into my homework. As a result, my workflow is very fast!\nI deliberately chose to map the buttons to shortcuts for which I wouldn\u0026rsquo;t want to move my pen. For example, if I\u0026rsquo;m writing a word and I mess up, I can undo my mistake without having to change my pen position, so I can rewrite it immediately. I can select and move a group of objects with the lasso without having to move away. I have come to realize that it is much faster than a traditional tablet because of these shortcut keys. Maybe an iPad would be better with some kind of shortcut keypad\u0026hellip; looks like that exists too.\nConveniently, the tablet has little rubber feet that fit perfectly over my laptop keyboard and trackpad. I use a 15.6 inch Lenovo ThinkPad, and by complete coincidence, the Huion tablet is exactly the same size. It\u0026rsquo;s perfect for when I\u0026rsquo;m taking notes in a lecture hall or the ACME study room is too full for me to be able to really spread out.\nWhen I do have room to spread out, I take advantage of the space.\nPeople ask me about my tablet all the timeâ€”but usually it\u0026rsquo;s after they\u0026rsquo;ve noticed my mouse. Like most people, I used to use a normal one. I\u0026rsquo;m relatively tall, so I have big hands. Something about gripping a tiny plastic mouse, scrunching up my hand, and moving it around for hours like that made my hand and wrist start to hurt. It would be a real bummer to get carpal tunnel syndrome, so I took action before it was too late and started looking into alternatives to the usual mouse.\nThat\u0026rsquo;s when I discovered trackballs. What a revelation! I picked up a Kensington Orbit a year and a half ago (a couple months after getting the tablet, I think) and have never looked back. My wrist and hand have never hurt since.\nBecause it\u0026rsquo;s larger and wider than a normal mouse, it opens my hand up. That\u0026rsquo;s the real key, I think. It also takes up a lot less space because it stays in place unlike a mouse, which is a huge plus when I\u0026rsquo;m in a crowded study room or computer lab. I love the tactile feel of moving the ball around and scrolling with the ring, it feels more natural to me. It took time to get used to it, but these days I bring the thing everywhere.\nThis setup has been a game-changer for me as a student. Itâ€™s affordable, reliable, and works well with my workflow. Sometimes the best solutions are the ones you figure out by thinking a little outside the box. If nothing else, itâ€™s been fun to experiment and find what works best for me.\n","date":"25 November 2024","permalink":"/posts/tablet-and-mouse/","section":"Posts","summary":"","title":"A cheaper alternative to tablets, and the mouse to rule them all"},{"content":"This project is hosted on GitHub.\nIn the BYU Biophysics research group, we spend much of our time working with 3-dimensional images of bacteria called tomograms. Researchers have spent a lot of time looking for structures in these tomograms, and save their findings in annotation files. tomogram_datasets makes it easier to navigate the web of tomograms and annotations we have, simplifying analysis and dataset creation. While it\u0026rsquo;s something I\u0026rsquo;m still working on, I use it every day in my research.\nThe code is hosted on GitHub.\nIt puts tomograms and their respective annotations into an object-oriented framework, so that accessing attributes of a particular tomogram, like annotations, supercomputer filepath, or header data, is quick and easy. This is primarily to facilitate the creation and analysis of competition datasets. Our group has already done a couple Kaggle competitions internally at BYU, and as we prepare to launch our first worldwide competition, I found myself in need of an easier way to work with our tomograms.\nThe project was also an opportunity to practice robust coding practices (implementing unit tests, thorough documentation), scripting (file management on a supercomputer, file loading), as well as data processing.\nIn addition, I learned a lot about Python libraries making this, like how to define the dependencies of my project so users could install it without having to think about that. I think my favorite part was learning to generate a documentation site automatically from the code\u0026rsquo;s docstrings using MkDocs. Seeing it update itself as I added features was fascinating.\n","date":"13 November 2024","permalink":"/projects/tomogram-datasets/","section":"Projects","summary":"","title":"tomogram-datasets"},{"content":" This project is hosted on GitHub.\nBrief summary #While looking for ways to work with and extract information from cryo-ET tomograms (noisy 3D images) in the summer of 2024, I read a book about ways of making the most of messy data using linear algebra and optimization. In the process, I learned about Robust Principal Component Analysis (RPCA), which can be performed with Principal Component Pursuit (PCP).\nHere I apply Principal Component Pursuit to a video I shot in my lab. Using this method, I am able to extract the background and foreground of a video, using nothing more than linear algebra and a simple convex optimization problem.\nYour browser does not support the video tag. The top video is the original, which I shot of myself in my lab. The middle video is the \u0026ldquo;background\u0026rdquo; (not moving) component of the video. The bottom video is the \u0026ldquo;foreground\u0026rdquo; (moving) component of the video. This project is inspired by a common application of PCP: identifying video segments in surveillance camera feeds where something of interest is happening. The video above simulates this in just a few seconds.\nI have yet to successfully apply this method to tomograms, but my advisor and I thought it was fascinating, so we feel like we succeeded anyway! I implemented the method in Julia, a language I am coming to love. See my implementation code on GitHub.\nIf you would like to learn more details, read the longer summary below.\nLonger summary #What is this? #This is an implementation and demonstration of Principal Component Pursuit, a way to solve the Robust Principal Component Analysis problem, which is here applied to a short video I shot.\nWhat am I seeing? #I recorded the top video in my lab. I wanted a video with a mostly static background and something moving in the foreground.\nUsing the method, which I will describe next, I separate the video into a static component and a moving component. The static component is the middle video. The moving component is the bottom video. In other words, the bottom video added to the middle video yields the top video.\nHow does it work? #Each frame of a grayscale video can be thought of as a matrix of grayscale values. For each frame, I take that matrix and flatten it into a vector. Thus, each frame of the video can be represented as a long vector with as many elements as there are pixels in a frame. I will call this vector a \u0026ldquo;frame vector\u0026rdquo;.\nBy representing the frames as vectors, the entire video can itself be represented as a matrix. This is done by stacking all of the frame vectors side by side into a huge matrix, with as many columns as there are frames in the video. I will call this matrix a \u0026ldquo;video matrix\u0026rdquo;.\nIn a video with a mostly static background and something moving in the foreground, the video matrix is almost low rank, since the frame vectors are mostly the same (since most of the pixels don\u0026rsquo;t change). But it isn\u0026rsquo;t, because of the movement in the foreground. Nevertheless, we can find a video matrix that is nearly equal to the original video matrix but is in fact low rank. In simpler terms, we can extract the background of the video.\nLet \\(\\bf{Y}\\) be the original video matrix. We want a video matrix \\(\\bf{L}\\) that is nearly equal to \\(\\bf{Y}\\), differing only by a sparse (meaning most of the elements are 0) matrix \\(\\bf{L}\\). In other words, we want to find \\(\\bf{L}\\) and \\(\\bf{S}\\) such that \\(\\bf{Y} = \\bf{L} + \\bf{S}\\), while minimizing \\(\\text{rank}(\\bf{L})\\) and \\(\\Vert\\bf{S}\\Vert_0\\) (where \\(\\Vert\\cdot\\Vert_0\\) gives the number of non-zero elements of its input, which technically is not a proper mathematical norm).\nOne could set this up as an optimization problem\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\text{rank}(\\bf{L}) + \\lambda \\Vert\\bf{S}\\Vert_0 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S} \\end{aligned} $$\nfor some tuning parameter \\(\\lambda \\in \\mathbb{R}\\), but the objective is not convex, making this very difficult to solve.\nRather, we set up the problem using convex surrogate norms:\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\Vert\\bf{L}\\Vert_* + \\lambda \\Vert\\bf{S}\\Vert_1 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S}, \\end{aligned} $$\nwhere \\(\\Vert \\cdot \\Vert_*\\) is the nuclear norm, meaning, the sum of the singular values of the input matrix, and \\(\\Vert \\cdot \\Vert_1\\) is the standard matrix 1-norm (the maximum column sum). (See \u0026ldquo;Note: Why these norms?\u0026rdquo; below.)\nThis new problem is convex! It is easy to solve with off-the-shelf convex optimizers. I have opted to implement the optimizer myself, but other libraries like CVXPY (in Python) or Convex.jl (for Julia) should work fine.\nBy solving\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\Vert\\bf{L}\\Vert_* + \\lambda \\Vert\\bf{S}\\Vert_1 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S}, \\end{aligned} $$\nwe find video matrices \\(\\bf{L}\\) and \\(\\bf{S}\\) that, for all intents and purposes, separate the original video matrix \\(\\bf{Y}\\) into \u0026ldquo;background\u0026rdquo; and \u0026ldquo;foreground\u0026rdquo; components respectively. Problem solved!\nNote: Why these norms? #First, we want to minimize the rank of \\(\\bf{L}\\). When the rank of \\(\\bf{L}\\) is minimized, we hope that most of its singular values are zero. Perhaps this will shed some intuition on why the nuclear norm makes sense here.\nSecond, we want to minimize the number of nonzero elements of \\(\\bf{S}\\) (what I called \\(\\Vert \\cdot \\Vert_0\\) above). When the number of nonzero elements of \\(\\bf{S}\\) is minimized, we would hope that the maximum column sum of \\(\\bf{S}\\) is quite small. Hopefully this clarifies why the 1-norm is a reasonable choice.\nFor more formal justification for these choices of norm, consult Wright and Ma\u0026rsquo;s textbook High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications.\n","date":"13 November 2024","permalink":"/projects/principal-component-pursuit/","section":"Projects","summary":"","title":"Principal Component Pursuit"},{"content":"Applied Mathematics student at Brigham Young University\nInterested in computer vision, optimization, machine learning, etc.\nLearn more about me here. ","date":null,"permalink":"/","section":"Home","summary":"","title":"Home"},{"content":"This project is hosted on GitHub.\nA helix plotted within a tomogram. I spend a lot of time working with cryo-electron tomograms. They\u0026rsquo;re huge, noisy, three-dimensional images that can take up gigabytes of storage apiece.\nNaturally, visualizing these images is a painâ€”most of the options are desktop applications like IMOD. napari is marvelous, but heavier than I usually need. All the time I found myself in a Jupyter notebook working doing data analysis on tomograms and I just wanted a quick snapshot of what the volume looks like, perhaps with a couple keypoints marked. And I wanted it to be dead-simple, so that with a single function call and a few seconds I could see what sort of image I was working with. So I wrote visualize_voxels.\nIt was my first time writing a Python library. Really, calling it a library is a stretch, because it only delivers to the user one function (visualize). But because I work in many environments (on my laptop, in online Jupyter notebooks, through SSH on BYU\u0026rsquo;s supercomputer), I wanted it to be easily installable via pip.\nThe result was exactly what I needed. Given a NumPy array of scalars arr, simply calling visualize(arr) produces a visualization like the one displayed below in secondsâ€”quick, simple, effective. Then I started adding other useful features, like the ability to mark points in the volume, and change the size, speed, and resolution of the visualization, among other little things. I made it work seamlessly in both .py scripts as well as notebooks.\nThis code (which leverages tomogram-datasets as well) visualizes the tomogram with the third-largest number of flagellar motors in our supercomputer:\nfrom tomogram_datasets import all_fm_tomograms import numpy as np from visualize_voxels import visualize as viz tomos = all_fm_tomograms() n_flagellar_motors = [len(tomo.annotation_points()) for tomo in tomos] super_tomo = tomos[np.argsort(n_flagellar_motors)[-3]] viz( super_tomo.get_data(), marks=super_tomo.annotation_points(), markalpha=0.5, axis=0, slices=np.linspace(80, 320, 100), fps=16 ) Try playing with it here.\n","date":"13 November 2024","permalink":"/projects/visualize-voxels/","section":"Projects","summary":"","title":"visualize-voxels"},{"content":"I\u0026rsquo;m Matthew Ward, a student in the Applied and Computational Mathematics Emphasis (ACME) program at Brigham Young University. I am also a member of the BYU Biophysics Group and soon to be a data engineer intern working on lidar technologies at 3DEO, Inc.\nI focus on computer vision (especially for volumetric images these days), optimization, and the mathematics behind data science and machine learning.\nMy wife and I When I\u0026rsquo;m not studying, I like to sing my wife songs at the piano. She and I play strings together in a non-music-major string orchestra at BYU. We love cooking and baking too.\n","date":null,"permalink":"/about/","section":"Home","summary":"","title":"About me"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"I would love to get in touch with you! Contact me here and I\u0026rsquo;ll get back to you soon. (This form will send me an email.)\nEmail Message Send Message\n","date":null,"permalink":"/contact/","section":"Home","summary":"","title":"Contact"},{"content":" This resume page is interactive. Click on the green links to see relevant examples of my work.\nEducation #BS, Applied and Computational Mathematics Emphasis (ACME)\nBrigham Young University | Provo, Utah\nApril 2026\nConcentration: Data Science \u0026amp; Machine Learning GPA: 3.97 Academic Scholarship (3 years) Skills #Programming # Proficient in NumPy Pandas Python scikit-learn PyTorch Julia C\u0026#43;\u0026#43; SQL Java Unix Shell Experience with HTML/CSS VBA SystemVerilog Object-oriented programming Database management High-performance computing Git / GitHub Excel Data Science \u0026amp; Machine Learning # Data visualization Data analysis Digital image processing Computer vision Machine learning Deep learning Numerical and dynamic optimization Mathematics \u0026amp; Algorithms # Dynamic systems Numerical linear algebra Mathematical statistics Curriculum in 2025 #Bayesian modeling, hidden Markov models, state-space models, ARIMA models, optimal control, control theory\nExperience #Research Assistant\nBrigham Young University â€” Biophysics Simulation Group | Provo, Utah\nApril 2024â€“Present\nCollaborate with a multidisciplinary team of physicists, biochemists, mathematicians, and computer scientists to develop and evaluate particle picking, image segmentation, and template matching algorithms and pipelines in cryo- electron tomography Analyze, visualize and present statistical findings related to a vast database of over 50 terabytes of three-dimensional images Write and optimize Python and Julia code for advanced image processing and object recognition tasks using algorithms such as Canny edge detection, SLIC superpixels, U-Net, etc. Leverage supercomputer resources to run image processing tasks, improving computational efficiency and enabling the analysis of high-resolution data Maintain code integrity using Git and GitHub Financial and Executive Secretary\nThe Church of Jesus Christ of Latter-day Saints | Cali, Valle del Cauca, Colombia\nMarch 2022â€“February 2023\nStreamlined financial systems and managed travel plans for 150 full-time representatives Designed and developed a VBA-based software tool to categorize 10,000 poorly formatted addresses into geographically organized lists which updated dynamically based on location and user input Provided 24/7 logistical, technological, and financial support to the president of the organization Projects #See the Projects page on this site.\n","date":null,"permalink":"/resume/","section":"Home","summary":"","title":"Resume"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]