[{"content":"","date":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":"This project is hosted on GitHub.\nIn the BYU Biophysics research group, we spend much of our time working with 3-dimensional images of bacteria called tomograms. Researchers have spent a lot of time looking for structures in these tomograms, and save their findings in annotation files. tomogram_datasets makes it easier to navigate the web of tomograms and annotations we have, simplifying analysis and dataset creation. While it\u0026rsquo;s something I\u0026rsquo;m still working on, I use it every day in my research.\nThe code is hosted on GitHub.\nIt puts tomograms and their respective annotations into an object-oriented framework, so that accessing attributes of a particular tomogram, like annotations, supercomputer filepath, or header data, is quick and easy. This is primarily to facilitate the creation and analysis of competition datasets. Our group has already done a couple Kaggle competitions internally at BYU, and as we prepare to launch our first worldwide competition, I found myself in need of an easier way to work with our tomograms.\nThe project was also an opportunity to practice robust coding practices (implementing unit tests, thorough documentation), scripting (file management on a supercomputer, file loading), as well as data processing.\nIn addition, I learned a lot about Python libraries making this, like how to define the dependencies of my project so users could install it without having to think about that. I think my favorite part was learning to generate a documentation site automatically from the code\u0026rsquo;s docstrings using MkDocs. Seeing it update itself as I added features was fascinating.\n","date":"13 November 2024","permalink":"/projects/tomogram-datasets/","section":"Projects","summary":"","title":"tomogram-datasets"},{"content":" This project is hosted on GitHub.\nBrief summary #While looking for ways to work with and extract information from cryo-ET tomograms (noisy 3D images) in the summer of 2024, I read a book about ways of making the most of messy data using linear algebra and optimization. In the process, I learned about Robust Principal Component Analysis (RPCA), which can be performed with Principal Component Pursuit (PCP).\nHere I apply Principal Component Pursuit to a video I shot in my lab. Using this method, I am able to extract the background and foreground of a video, using nothing more than linear algebra and a simple convex optimization problem.\nThe top video is the original, which I shot of myself in my lab. The middle video is the \u0026ldquo;background\u0026rdquo; (not moving) component of the video. The bottom video is the \u0026ldquo;foreground\u0026rdquo; (moving) component of the video. This project is inspired by a common application of PCP: identifying video segments in surveillance camera feeds where something of interest is happening. The video above simulates this in just a few seconds.\nI have yet to successfully apply this method to tomograms, but my advisor and I thought it was fascinating, so we feel like we succeeded anyway! I implemented the method in Julia, a language I am coming to love. See my implementation code on GitHub.\nIf you would like to learn more details, read the longer summary below.\nLonger summary #What is this? #This is an implementation and demonstration of Principal Component Pursuit, a way to solve the Robust Principal Component Analysis problem, which is here applied to a short video I shot.\nWhat am I seeing? #I recorded the top video in my lab. I wanted a video with a mostly static background and something moving in the foreground.\nUsing the method, which I will describe next, I separate the video into a static component and a moving component. The static component is the middle video. The moving component is the bottom video. In other words, the bottom video added to the middle video yields the top video.\nHow does it work? #Each frame of a grayscale video can be thought of as a matrix of grayscale values. For each frame, I take that matrix and flatten it into a vector. Thus, each frame of the video can be represented as a long vector with as many elements as there are pixels in a frame. I will call this vector a \u0026ldquo;frame vector\u0026rdquo;.\nBy representing the frames as vectors, the entire video can itself be represented as a matrix. This is done by stacking all of the frame vectors side by side into a huge matrix, with as many columns as there are frames in the video. I will call this matrix a \u0026ldquo;video matrix\u0026rdquo;.\nIn a video with a mostly static background and something moving in the foreground, the video matrix is almost low rank, since the frame vectors are mostly the same (since most of the pixels don\u0026rsquo;t change). But it isn\u0026rsquo;t, because of the movement in the foreground. Nevertheless, we can find a video matrix that is nearly equal to the original video matrix but is in fact low rank. In simpler terms, we can extract the background of the video.\nLet \\(\\bf{Y}\\) be the original video matrix. We want a video matrix \\(\\bf{L}\\) that is nearly equal to \\(\\bf{Y}\\), differing only by a sparse (meaning most of the elements are 0) matrix \\(\\bf{L}\\). In other words, we want to find \\(\\bf{L}\\) and \\(\\bf{S}\\) such that \\(\\bf{Y} = \\bf{L} + \\bf{S}\\), while minimizing \\(\\text{rank}(\\bf{L})\\) and \\(\\Vert\\bf{S}\\Vert_0\\) (where \\(\\Vert\\cdot\\Vert_0\\) gives the number of non-zero elements of its input, which technically is not a proper mathematical norm).\nOne could set this up as an optimization problem\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\text{rank}(\\bf{L}) + \\lambda \\Vert\\bf{S}\\Vert_0 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S} \\end{aligned} $$\nfor some tuning parameter \\(\\lambda \\in \\mathbb{R}\\), but the objective is not convex, making this very difficult to solve.\nRather, we set up the problem using convex surrogate norms:\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\Vert\\bf{L}\\Vert_* + \\lambda \\Vert\\bf{S}\\Vert_1 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S}, \\end{aligned} $$\nwhere \\(\\Vert \\cdot \\Vert_*\\) is the nuclear norm, meaning, the sum of the singular values of the input matrix, and \\(\\Vert \\cdot \\Vert_1\\) is the standard matrix 1-norm (the maximum column sum). (See \u0026ldquo;Note: Why these norms?\u0026rdquo; below.)\nThis new problem is convex! It is easy to solve with off-the-shelf convex optimizers. I have opted to implement the optimizer myself, but other libraries like CVXPY (in Python) or Convex.jl (for Julia) should work fine.\nBy solving\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\Vert\\bf{L}\\Vert_* + \\lambda \\Vert\\bf{S}\\Vert_1 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S}, \\end{aligned} $$\nwe find video matrices \\(\\bf{L}\\) and \\(\\bf{S}\\) that, for all intents and purposes, separate the original video matrix \\(\\bf{Y}\\) into \u0026ldquo;background\u0026rdquo; and \u0026ldquo;foreground\u0026rdquo; components respectively. Problem solved!\nNote: Why these norms? #First, we want to minimize the rank of \\(\\bf{L}\\). When the rank of \\(\\bf{L}\\) is minimized, we hope that most of its singular values are zero. Perhaps this will shed some intuition on why the nuclear norm makes sense here.\nSecond, we want to minimize the number of nonzero elements of \\(\\bf{S}\\) (what I called \\(\\Vert \\cdot \\Vert_0\\) above). When the number of nonzero elements of \\(\\bf{S}\\) is minimized, we would hope that the maximum column sum of \\(\\bf{S}\\) is quite small. Hopefully this clarifies why the 1-norm is a reasonable choice.\nFor more formal justification for these choices of norm, consult Wright and Ma\u0026rsquo;s textbook High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications.\n","date":"13 November 2024","permalink":"/projects/principal-component-pursuit/","section":"Projects","summary":"","title":"Principal Component Pursuit"},{"content":"Applied Mathematics student at Brigham Young University\n","date":null,"permalink":"/","section":"Home","summary":"","title":"Home"},{"content":"This project is hosted on GitHub.\nA helix plotted within a tomogram. I spend a lot of time working with cryo-electron tomograms. They\u0026rsquo;re huge, noisy, three-dimensional images that can take up gigabytes of storage apiece.\nNaturally, visualizing these images is a pain—most of the options are desktop applications like IMOD. napari is marvelous, but heavier than I usually need. All the time I found myself in a Jupyter notebook working doing data analysis on tomograms and I just wanted a quick snapshot of what the volume looks like, perhaps with a couple keypoints marked. And I wanted it to be dead-simple, so that with a single function call and a few seconds I could see what sort of image I was working with. So I wrote visualize_voxels.\nIt was my first time writing a Python library. Really, calling it a library is a stretch, because it only delivers to the user one function (visualize). But because I work in many environments (on my laptop, in online Jupyter notebooks, through SSH on BYU\u0026rsquo;s supercomputer), I wanted it to be easily installable via pip.\nThe result was exactly what I needed. Given a NumPy array of scalars arr, simply calling visualize(arr) produces a visualization like the one displayed below in seconds—quick, simple, effective. Then I started adding other useful features, like the ability to mark points in the volume, and change the size, speed, and resolution of the visualization, among other little things. I made it work seamlessly in both .py scripts as well as notebooks.\nThis code (which leverages tomogram-datasets as well) visualizes the tomogram with the third-largest number of flagellar motors in our supercomputer:\nfrom tomogram_datasets import all_fm_tomograms import numpy as np from visualize_voxels import visualize as viz tomos = all_fm_tomograms() n_flagellar_motors = [len(tomo.annotation_points()) for tomo in tomos] super_tomo = tomos[np.argsort(n_flagellar_motors)[-3]] viz( super_tomo.get_data(), marks=super_tomo.annotation_points(), markalpha=0.5, axis=0, slices=np.linspace(80, 320, 100), fps=16 ) Try playing with it here.\n","date":"13 November 2024","permalink":"/projects/visualize-voxels/","section":"Projects","summary":"","title":"visualize-voxels"},{"content":"I\u0026rsquo;m Matthew Ward, a student in the Applied and Computational Mathematics Emphasis (ACME) program at Brigham Young University, and a member of the BYU Biophysics Group.\nI am especially interested in computer vision, optimization, and the mathematics behind data science and machine learning.\nWhen I\u0026rsquo;m not studying, I like to sing my wife songs at the piano. She and I play strings together in a non-music-major string orchestra at BYU. We love cooking and baking too.\n","date":null,"permalink":"/about/","section":"Home","summary":"","title":"About me"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"I would love to get in touch with you! Contact me here and I\u0026rsquo;ll get back to you soon.\nEmail Message Send Message\n","date":null,"permalink":"/contact/","section":"Home","summary":"","title":"Contact"},{"content":" Education #BS, Applied and Computational Mathematics Emphasis (ACME)\nBrigham Young University | Provo, Utah\nApril 2026\nConcentration: Data Science \u0026amp; Machine Learning GPA: 3.98 Academic Scholarship (3 years) Skills #Programming # Python NumPy Pandas scikit-learn PyTorch Julia SQL C\u0026#43;\u0026#43; Java HTML/CSS Unix Shell Object-oriented programming Database management High-performance computing Git / GitHub Excel Data Science \u0026amp; Machine Learning # Data visualization Data analysis Digital image processing Computer vision Machine learning Deep learning Numerical and dynamic optimization Mathematics \u0026amp; Algorithms # Dynamic systems Numerical linear algebra Mathematical statistics Curriculum in 2025 #Bayesian modeling, hidden Markov models, state-space models, ARIMA models, optimal control, control theory\nExperience #Research Assistant\nBrigham Young University — Biophysics Simulation Group | Provo, Utah\nApril 2024–Present\nCollaborate with a multidisciplinary team of physicists, biochemists, mathematicians, and computer scientists to develop and evaluate particle picking, image segmentation, and template matching algorithms and pipelines in cryo- electron tomography Analyze, visualize and present statistical findings related to a vast database of over 50 terabytes of three-dimensional images Write and optimize Python and Julia code for advanced image processing and object recognition tasks using algorithms such as Canny edge detection, SLIC superpixels, U-Net, etc. Leverage supercomputer resources to run image processing tasks, improving computational efficiency and enabling the analysis of high-resolution data Maintain code integrity using Git and GitHub Financial and Executive Secretary\nThe Church of Jesus Christ of Latter-day Saints | Cali, Valle del Cauca, Colombia\nMarch 2022–February 2023\nStreamlined financial systems and managed travel plans for 150 full-time representatives Designed and developed a VBA-based software tool to categorize 10,000 poorly formatted addresses into geographically organized lists which updated dynamically based on location and user input Provided 24/7 logistical, technological, and financial support to the president of the organization Projects #See the Projects page on this site.\n","date":null,"permalink":"/resume/","section":"Home","summary":"","title":"Resume"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]