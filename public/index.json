[{"content":"This project is a paper with associated code.\nAs a student in BYU\u0026rsquo;s Applied and Computational Mathematics Emphasis (ACME), this semester I took a class in modeling with data and uncertainty. In it my peers and I learned (among many other things) about the theoretical underpinnings of various classification and regression techniques. To enhance our practical understanding of machine learning, we were assigned an open-ended group project—given a list of a dozen or so datasets from Kaggle, we were pick a dataset, ask some interesting question about the data, and attempt to answer it with the techniques we\u0026rsquo;d learned this semester. Then we were to summarize our findings in a five-page paper.\nI wrote the paper with my classmate Rebecca Gee. She focused primarily on data preparation and understanding what the features in the dataset actually meant (what in the world is \u0026ldquo;Income Composition of Resources\u0026rdquo;? She figured it out), and I focused on the feature engineering and machine learning methods.\nI usually manage my code with Git and GitHub, but this time I opted for a Jupyter notebook in Google Colab to avoid training the models on my system. View the code here. It proved challenging to export the large notebook to other formats (.pdf, .html). Troubleshooting would\u0026rsquo;ve been easier had I coded in a more robust way, by saving trained models and such. The time I spent wrestling with the notebook export reminded me to be more careful next time, in spite of any looming deadlines.\nThe original paper is a .pdf document, but for a better reading experience, I\u0026rsquo;ll copy it as best I can below.\nPredicting Future Life Expectancy with Present Data #Matthew Ward, Rebecca Gee #\nAbstract. Predicting life expectancy is crucial for understanding global health trends and guiding policy decisions. We aim to predict the life expectancy of a country five years into the future using socio-economic and health-related features. After engineering features to normalize trends and focus on deviations from global averages, we employ several machine learning models, including Random Forest and Gradient Boosting, to make predictions. Initial results demonstrate modest predictive performance when evaluated on unseen countries, with features like income composition and vaccination rates contributing significantly. However, cluster-based evaluation reveals that the models struggle to generalize across diverse regions, highlighting challenges in capturing global heterogeneity.\n1. Research question and overview of the data #Using the WHO Life Expectancy dataset, how well can we predict the life expectancy of a given country in 5 years from now?\nPrevious research done with similar datasets used a variety of techniques. Lipesa, et al. used extreme gradient boosting (XGBoost) and pointed to thinness, schooling, infant deaths and BMI as leading factors in Life Expectancy. Gill et al. used linear regression to find a correlation between adult mortality and GDP per capita.\nThe Life Expectancy Dataset contains data from countries over a fifteen year period. Below we outline a few.\nFeature Explanation Life Expectancy Average national life expectancy (years) Alcohol Alcohol consumed per capita (liters) Percentage expenditure Government expenditure on health (% of GDP) Polio Percent population vaccinated against polio 15 other features. See Appendix A. This data captures a variety of different ideas useful in predicting life expectancy, for instance, immunizations, BMI and adult mortality seem to be good indicators of life expectancy. Other datasets with less information or less countries would be less suited to this model.\nOne weakness of the dataset is an abundance of missing, incomplete or bad data, which we can correct or impute. The columns associated with diseases often use different metrics, and are therefore hard to compare. For example, it would be hard to ask which disease has the biggest impact on life expectancy, since each disease is not measured the same way.\nWe will use this dataset to predict life expectancy in the future, and to determine what components most contribute to life expectancy.\n2. Data Cleaning / Feature Engineering #There were four countries (North and South Korea, Sudan, and South Sudan) that lacked a substantial portion of their data, necessitating their removal. We also dropped the Population column because it was missing data and was not a key feature of the model.\nWe also noticed that there were problems with the ``GDP\u0026quot; data column. Much of it was missing or inaccurate. For this reason, we chose to take the GDP from another dataset, which we verified had accurate and complete numbers by comparing a random subset of the numbers to those found by the World Bank.\nWe imputed the rest of the missing data using an imputer built with the k-Nearest Neighbors algorithm (KNNImputer from scikit-learn).\nWe hope that given statistics for a year of data, we can predict life expectancy five years later. Now, to do this, we will not simply train a model to predict life expectancy, since it is increasing worldwide.\nFigure 1. Average life expectancy is increasing worldwide. Thus we create new features that will more accurately indicate if the model is learning to predict meaningful trends.\nDeviation from worldwide mean. Unlike life expectancy worldwide, this feature will on average be flat. Deviation from worldwide mean in five years. Of course, it is not possible to calculate this feature for the last five years represented in the dataset. After calculating this feature wherever it is possible, we drop rows in the last five years. Five-year change in deviation from worldwide mean. This is the output of our model. It should be relatively flat and close to zero, unlike a five-year change in life expectancy, which is normally greater than zero. 3. Data Visualization and Basic Analysis # Figure 2. Life expectancy is not always stable. We have already seen that on average, life expectancy is increasing, however, it is not always stable. In some countries it wildly fluctuates, and in others it is quite continuous (Figure 2). Peru is an example of a country with a more or less constant rate of change in its life expectancy, matching the trend of the worldwide mean quite nicely. However, we see that in Spain, there was a sharp increase for about two years, after which it resumed it\u0026rsquo;s previous value. Life expectancy in Iraq varies wildly around the mean and Rwanda increases steadily with a couple irregularities where life expectancy was much higher than one might expect.\nLife expectancy prediction is not a trivial problem. The data often contradicts common assumptions about what helps and hurts, possibly because of conflating variables (see Figure 3).\nFigure 3. Prediction of life expectancy is not trivial. 4. Learning Algorithms and In-depth Analysis #We split the dataset into training (80%) and testing (20%) sets, grouping by country.\nWe trained models on the deviation from the worldwide mean in five years, a feature we engineered as described in Section 2.\nUsing Bayesian optimization via Optuna, we optimized hyperparameters for Random Forest, Gradient Boosted, XGBoost, as well as Ridge regression models with cross-validation, again grouping data by country. As can be seen in our code, after hyperparameter tuning, the XGBoost regressor had the best average validation score during cross-validation. On the test set it had a coefficient of correlation of 0.30, indicating that while its performance was not stellar, it did learn something meaningful—after all, it was scored on data from countries it had never seen before.\nFigure 4. The model is able to make some meaningful predictions (top), but sometimes fails (bottom). Since the model uses data from a given year to predict the life expectancy five years from then, the predicted line begins in 2005 and continues to 2020. The outputs of the model give the change in life expectancy over 5 years, so adding those to the current life expectancy gives us the orange line as shown. The five most important features for the best model were (in order) present life expectancy (naturally), HIV/AIDS cases, Polio vaccinations, ICOR, and Diphtheria vaccinations (see Appendix A).\nHowever, changing the way we split the data for training and testing proves that our initial model is not generalizable. In our new testing set, we clustered the data into eight clusters by performing PCA on the normalized data, using k-means clustering to refine the groups, and merging clusters that were too small.\nFigure 5. The clusters visualized using the first two principal components. See Appendix B. We used these clusters as folds in cross-validation, except for Cluster 4, which we randomly selected and set apart as a test set. This means that the models saw plenty of data, but had to use what they learned on countries that were substantially different than anything they had previously seen.\nAs before, we trained various models on the rest of the data, tuning hyperparameters for various models with cross-validation to optimize performance. Using the best model (XGBoost), we obtained a coefficient of correlation of 0.15 on the test set. Compared to the previous coefficient of correlation of 0.30, this is drastically worse.\nIt is possible to predict future life expectancy in a target country given data from countries similar to it. However, predicting life expectancy in a country that is different from the rest is much more difficult.\n5. Ethical Implications and Conclusions #Predicting life expectancy in the future has several important ethical implications. The potential for future humanitarian work and prevention of decreasing life expectancy is powerful. The model is quite harmless, but there are a few ways it could be used negatively.\nFor instance, in war, if an enemy had enough data, they could identify key statistics to weaken the other country\u0026rsquo;s health using methods like these. In addition, overreacting to the model could create a self-fulfilling prophecy.\nTo ensure the model functions effectively, it requires data from a diverse range of countries. Organizations like the WHO must ensure this data is managed responsibly and securely.\nIn conclusion, predicting life expectancy in the future is possible with sufficient data. In order to predict the life expectancy, a model with many different countries is needed, and the results can be relatively accurate on a short time period.\nAppendix #A. Features of Dataset #(in no particular order)\nStatus (Developed/Developing) Life Expectancy (age) Adult Mortality (Probability of dying between 15-60 years of age per 1000) Infant deaths (number per 1000) Alcohol (consumed per capita in liters) Percentage expenditure (Expenditure on health percentage of GDP) Hepatitis B (Percentage immunizations) Measles (number of cases per 1000) BMI (average of population) Under 5 deaths (number per 1000) Polio (Percentage immunizations) Total expenditure (percentage expenditure on health of total government expenditure) Diphtheria (Percentage immunizations) HIV/AIDS (Deaths per 1000 live births) GDP (per capita in USD), Population (of country) Thinness 1-19 years (prevalence of thinness) Thinness 5-9 years (prevalence of thinness) Income Composition of Resources (ICOR) (Human Development Index in terms of ICOR) Schooling (number of years of schooling) B. Countries in Clusters #Cluster 1:\nAfghanistan, Angola, Central African Republic, Chad, China, Congo, Democratic Republic of the Congo, Equatorial Guinea, Ethiopia, Gabon, Guinea, Haiti, India, Lao People\u0026rsquo;s Democratic Republic, Liberia, Niger, Nigeria, Somalia, Uganda\nCluster 2:\nAlbania, Antigua and Barbuda, Armenia, Bahrain, Belize, Brunei Darussalam, Cabo Verde, Colombia, Cuba, Egypt, El Salvador, Fiji, Grenada, Guatemala, Guyana, Honduras, Iran (Islamic Republic of), Israel, Jamaica, Jordan, Kuwait, Kyrgyzstan, Libya, Malaysia, Maldives, Mauritius, Mexico, Mongolia, Morocco, Nicaragua, Oman, Panama, Paraguay, Peru, Qatar, Saint Vincent and the Grenadines, Sao Tome and Principe, Saudi Arabia, Seychelles, Singapore, Sri Lanka, Tajikistan, Thailand, The former Yugoslav republic of Macedonia, Tunisia, Turkmenistan, United Arab Emirates, Uzbekistan, Viet Nam\nCluster 3:\nAlgeria, Azerbaijan, Bolivia (Plurinational State of), Bosnia and Herzegovina, Costa Rica, Dominican Republic, Ecuador, Georgia, Iraq, Kiribati, Lebanon, Micronesia (Federated States of), Montenegro, Philippines, Samoa, Solomon Islands, Suriname, Syrian Arab Republic, Tonga, Trinidad and Tobago, Turkey, Ukraine, Vanuatu, Venezuela (Bolivarian Republic of)\nCluster 4:\nArgentina, Bahamas, Barbados, Belarus, Brazil, Bulgaria, Chile, Croatia, Cyprus, Czechia, Estonia, Finland, Greece, Hungary, Italy, Kazakhstan, Latvia, Lithuania, Malta, Poland, Portugal, Republic of Moldova, Romania, Russian Federation, Saint Lucia, Serbia, Slovakia, Slovenia, Spain, United Kingdom of Great Britain and Northern Ireland, United States of America, Uruguay\nCluster 5:\nAustralia, Austria, Belgium, Canada, Denmark, France, Germany, Iceland, Ireland, Japan, Luxembourg, Netherlands, New Zealand, Norway, Sweden, Switzerland\nCluster 6:\nBangladesh, Benin, Bhutan, Botswana, Burkina Faso, Burundi, Cambodia, Cameroon, Comoros, Côte d\u0026rsquo;Ivoire, Djibouti, Eritrea, Gambia, Ghana, Guinea-Bissau, Indonesia, Kenya, Lesotho, Madagascar, Malawi, Mali, Mauritania, Mozambique, Myanmar, Namibia, Nepal, Pakistan, Papua New Guinea, Rwanda, Senegal, Sierra Leone, South Africa, Swaziland, Timor-Leste, Togo, United Republic of Tanzania, Yemen, Zambia, Zimbabwe\n","date":"17 December 2024","permalink":"/projects/acme-volume-3-project/","section":"Projects","summary":"","title":"Predicting future life expectancy in countries using present data"},{"content":"","date":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":" As a student in applied mathematics, I spend much of my time writing code, taking lecture notes, and working out proofs, all at breakneck speed. Many of my peers use \\(\\LaTeX\\) to take notes and do homework. While I appreciate \\(\\LaTeX\\) and often use it for formal write-ups, I don\u0026rsquo;t love not being able to quickly draw diagrams, change colors, make up new notation on the fly, etc. Handwriting is infinitely flexible, and in my experience, much quicker when I\u0026rsquo;m trying to pump out my homework and move on.\nA notebook isn\u0026rsquo;t enough. I want my notes to be stored in the cloud. I want them to be organized and easily accessible. I want them to be stored in .pdf format so they\u0026rsquo;re future-proof. And I don\u0026rsquo;t want to buy a tablet. Why? Well, good tablets are expensive, I like my laptop, I don\u0026rsquo;t want to bring two devices to school, and I like being able to seamlessly copy snippets of code I\u0026rsquo;m writing and textbooks I\u0026rsquo;m referencing into my notes, among other things.\nAbout two years ago, the solution came to me. I had the idea when I was a missionary in Cali, Colombia, walking down the street just a few months before it was time for me to go home. I was an office secretary and I had just used the mission credit card to pay at a notary, and signed for it using a screenless credit card signature pad attached to the card reader.\nI thought as I walked, \u0026ldquo;why don\u0026rsquo;t they just take one of those and scale it up?\u0026rdquo; I was certain that such a product must exist.\nSure enough, it does—but it isn\u0026rsquo;t generally marketed to people like me. Many companies, the foremost of which is Wacom, manufacture screenless tablets with pressure-sensitive pens to do digital art and animation. The level of quality varies greatly. The fanciest one I\u0026rsquo;ve used has a screen and sells for $3,500 USD (The research group I currently work with has one to annotate tomograms—it\u0026rsquo;s a Wacom Cintiq Pro 27).\nI don\u0026rsquo;t have that kind of cash. So I saved a few hundred bucks and got one of these on Amazon for $50.\nThere\u0026rsquo;s really nothing special about this specific model (Huion H610PRO v2). It\u0026rsquo;s just a generic off-brand screenless Wacom tablet that connects to your computer through a USB cable. People usually use these to do digital art and animation. I have found that they work astoundingly well for notetaking as well. Neither the pen nor the tablet use batteries, so when I\u0026rsquo;m on the go, all I have to charge is my laptop.\nI\u0026rsquo;ve been using my tablet for a year and a half now, and it\u0026rsquo;s still working like new. I only had to replace the pen nib once because I was fiddling with it and lost it on the floor. Whoops!\nThe trickiest part about all this is how to set it up. Naturally with unstandardized hardware like a screenless tablet there are lots of driver issues and subtleties to keep in mind. In addition, I use Linux (Debian 12 with GNOME and KDE), so things are often a little less intuitive (but more fun). Huion (the manufacturer) makes a good driver, and I also like using OpenTabletDriver, an open-source alternative. Things get a little dicey with a multi-monitor setup but I think I\u0026rsquo;ve got it figured out now.\nI take notes in Xournal++. I love it. It\u0026rsquo;s everything I needed in notetaking software and more. My tablet has eight buttons on the left, which I\u0026rsquo;ve mapped to the following in Xournal++: undo, delete, pencil, zoom in, zoom out, new page, the lasso selection tool, and the hand tool. I also have an on-screen icon in my GNOME panel to take screenshots (Screenshot Tool), which is especially useful when I want to paste snippets from textbooks (all of which I access digitally) into my homework. As a result, my workflow is very fast!\nI deliberately chose to map the buttons to shortcuts for which I wouldn\u0026rsquo;t want to move my pen. For example, if I\u0026rsquo;m writing a word and I mess up, I can undo my mistake without having to change my pen position, so I can rewrite it immediately. I can select and move a group of objects with the lasso without having to move away. I have come to realize that it is much faster than a traditional tablet because of these shortcut keys. Maybe an iPad would be better with some kind of shortcut keypad\u0026hellip; looks like that exists too.\nConveniently, the tablet has little rubber feet that fit perfectly over my laptop keyboard and trackpad. I use a 15.6 inch Lenovo ThinkPad, and by complete coincidence, the Huion tablet is exactly the same size. It\u0026rsquo;s perfect for when I\u0026rsquo;m taking notes in a lecture hall or the ACME study room is too full for me to be able to really spread out.\nWhen I do have room to spread out, I take advantage of the space.\nPeople ask me about my tablet all the time—but usually it\u0026rsquo;s after they\u0026rsquo;ve noticed my mouse. Like most people, I used to use a normal one. I\u0026rsquo;m relatively tall, so I have big hands. Something about gripping a tiny plastic mouse, scrunching up my hand, and moving it around for hours like that made my hand and wrist start to hurt. It would be a real bummer to get carpal tunnel syndrome, so I took action before it was too late and started looking into alternatives to the usual mouse.\nThat\u0026rsquo;s when I discovered trackballs. What a revelation! I picked up a Kensington Orbit a year and a half ago (a couple months after getting the tablet, I think) and have never looked back. My wrist and hand have never hurt since.\nBecause it\u0026rsquo;s larger and wider than a normal mouse, it opens my hand up. That\u0026rsquo;s the real key, I think. It also takes up a lot less space because it stays in place unlike a mouse, which is a huge plus when I\u0026rsquo;m in a crowded study room or computer lab. I love the tactile feel of moving the ball around and scrolling with the ring, it feels more natural to me. It took time to get used to it, but these days I bring the thing everywhere.\nThis setup has been a game-changer for me as a student. It’s affordable, reliable, and works well with my workflow. Sometimes the best solutions are the ones you figure out by thinking a little outside the box. If nothing else, it’s been fun to experiment and find what works best for me.\n","date":"25 November 2024","permalink":"/posts/my-weird-workflow/","section":"Posts","summary":"","title":"A cheaper alternative to tablets, and the mouse to rule them all"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"This project is hosted on GitHub.\nIn the BYU Biophysics research group, we spend much of our time working with 3-dimensional images of bacteria called tomograms. Researchers have spent a lot of time looking for structures in these tomograms, and save their findings in annotation files. tomogram_datasets makes it easier to navigate the web of tomograms and annotations we have, simplifying analysis and dataset creation. While it\u0026rsquo;s something I\u0026rsquo;m still working on, I use it every day in my research.\nThe code is hosted on GitHub.\nIt puts tomograms and their respective annotations into an object-oriented framework, so that accessing attributes of a particular tomogram, like annotations, supercomputer filepath, or header data, is quick and easy. This is primarily to facilitate the creation and analysis of competition datasets. Our group has already done a couple Kaggle competitions internally at BYU, and as we prepare to launch our first worldwide competition, I found myself in need of an easier way to work with our tomograms.\nThe project was also an opportunity to practice robust coding practices (implementing unit tests, thorough documentation), scripting (file management on a supercomputer, file loading), as well as data processing.\nIn addition, I learned a lot about Python libraries making this, like how to define the dependencies of my project so users could install it without having to think about that. I think my favorite part was learning to generate a documentation site automatically from the code\u0026rsquo;s docstrings using MkDocs. Seeing it update itself as I added features was fascinating.\n","date":"13 November 2024","permalink":"/projects/tomogram-datasets/","section":"Projects","summary":"","title":"tomogram-datasets"},{"content":" This project is hosted on GitHub.\nBrief summary #While looking for ways to work with and extract information from cryo-ET tomograms (noisy 3D images) in the summer of 2024, I read a book about ways of making the most of messy data using linear algebra and optimization. In the process, I learned about Robust Principal Component Analysis (RPCA), which can be performed with Principal Component Pursuit (PCP).\nHere I apply Principal Component Pursuit to a video I shot in my lab. Using this method, I am able to extract the background and foreground of a video, using nothing more than linear algebra and a simple convex optimization problem.\nYour browser does not support the video tag. The top video is the original, which I shot of myself in my lab. The middle video is the \u0026ldquo;background\u0026rdquo; (not moving) component of the video. The bottom video is the \u0026ldquo;foreground\u0026rdquo; (moving) component of the video. This project is inspired by a common application of PCP: identifying video segments in surveillance camera feeds where something of interest is happening. The video above simulates this in just a few seconds.\nI have yet to successfully apply this method to tomograms, but my advisor and I thought it was fascinating, so we feel like we succeeded anyway! I implemented the method in Julia, a language I am coming to love. See my implementation code on GitHub.\nIf you would like to learn more details, read the longer summary below.\nLonger summary #What is this? #This is an implementation and demonstration of Principal Component Pursuit, a way to solve the Robust Principal Component Analysis problem, which is here applied to a short video I shot.\nWhat am I seeing? #I recorded the top video in my lab. I wanted a video with a mostly static background and something moving in the foreground.\nUsing the method, which I will describe next, I separate the video into a static component and a moving component. The static component is the middle video. The moving component is the bottom video. In other words, the bottom video added to the middle video yields the top video.\nHow does it work? #Each frame of a grayscale video can be thought of as a matrix of grayscale values. For each frame, I take that matrix and flatten it into a vector. Thus, each frame of the video can be represented as a long vector with as many elements as there are pixels in a frame. I will call this vector a \u0026ldquo;frame vector\u0026rdquo;.\nBy representing the frames as vectors, the entire video can itself be represented as a matrix. This is done by stacking all of the frame vectors side by side into a huge matrix, with as many columns as there are frames in the video. I will call this matrix a \u0026ldquo;video matrix\u0026rdquo;.\nIn a video with a mostly static background and something moving in the foreground, the video matrix is almost low rank, since the frame vectors are mostly the same (since most of the pixels don\u0026rsquo;t change). But it isn\u0026rsquo;t, because of the movement in the foreground. Nevertheless, we can find a video matrix that is nearly equal to the original video matrix but is in fact low rank. In simpler terms, we can extract the background of the video.\nLet \\(\\bf{Y}\\) be the original video matrix. We want a video matrix \\(\\bf{L}\\) that is nearly equal to \\(\\bf{Y}\\), differing only by a sparse (meaning most of the elements are 0) matrix \\(\\bf{L}\\). In other words, we want to find \\(\\bf{L}\\) and \\(\\bf{S}\\) such that \\(\\bf{Y} = \\bf{L} + \\bf{S}\\), while minimizing \\(\\text{rank}(\\bf{L})\\) and \\(\\Vert\\bf{S}\\Vert_0\\) (where \\(\\Vert\\cdot\\Vert_0\\) gives the number of non-zero elements of its input, which technically is not a proper mathematical norm).\nOne could set this up as an optimization problem\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\text{rank}(\\bf{L}) + \\lambda \\Vert\\bf{S}\\Vert_0 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S} \\end{aligned} $$\nfor some tuning parameter \\(\\lambda \\in \\mathbb{R}\\), but the objective is not convex, making this very difficult to solve.\nRather, we set up the problem using convex surrogate norms:\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\Vert\\bf{L}\\Vert_* + \\lambda \\Vert\\bf{S}\\Vert_1 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S}, \\end{aligned} $$\nwhere \\(\\Vert \\cdot \\Vert_*\\) is the nuclear norm, meaning, the sum of the singular values of the input matrix, and \\(\\Vert \\cdot \\Vert_1\\) is the standard matrix 1-norm (the maximum column sum). (See \u0026ldquo;Note: Why these norms?\u0026rdquo; below.)\nThis new problem is convex! It is easy to solve with off-the-shelf convex optimizers. I have opted to implement the optimizer myself, but other libraries like CVXPY (in Python) or Convex.jl (for Julia) should work fine.\nBy solving\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\Vert\\bf{L}\\Vert_* + \\lambda \\Vert\\bf{S}\\Vert_1 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S}, \\end{aligned} $$\nwe find video matrices \\(\\bf{L}\\) and \\(\\bf{S}\\) that, for all intents and purposes, separate the original video matrix \\(\\bf{Y}\\) into \u0026ldquo;background\u0026rdquo; and \u0026ldquo;foreground\u0026rdquo; components respectively. Problem solved!\nNote: Why these norms? #First, we want to minimize the rank of \\(\\bf{L}\\). When the rank of \\(\\bf{L}\\) is minimized, we hope that most of its singular values are zero. Perhaps this will shed some intuition on why the nuclear norm makes sense here.\nSecond, we want to minimize the number of nonzero elements of \\(\\bf{S}\\) (what I called \\(\\Vert \\cdot \\Vert_0\\) above). When the number of nonzero elements of \\(\\bf{S}\\) is minimized, we would hope that the maximum column sum of \\(\\bf{S}\\) is quite small. Hopefully this clarifies why the 1-norm is a reasonable choice.\nFor more formal justification for these choices of norm, consult Wright and Ma\u0026rsquo;s textbook High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications.\n","date":"13 November 2024","permalink":"/projects/principal-component-pursuit/","section":"Projects","summary":"","title":"Principal Component Pursuit"},{"content":"Applied Mathematics student at Brigham Young University\nInterested in computer vision, optimization, machine learning, etc.\nLearn more about me here. I'm looking for an internship in the summer of 2025! Review my resume and projects. Contact me via email at me@matthewward.info or through the Contact page on this site. ","date":null,"permalink":"/","section":"Home","summary":"","title":"Home"},{"content":"This project is hosted on GitHub.\nA helix plotted within a tomogram. I spend a lot of time working with cryo-electron tomograms. They\u0026rsquo;re huge, noisy, three-dimensional images that can take up gigabytes of storage apiece.\nNaturally, visualizing these images is a pain—most of the options are desktop applications like IMOD. napari is marvelous, but heavier than I usually need. All the time I found myself in a Jupyter notebook working doing data analysis on tomograms and I just wanted a quick snapshot of what the volume looks like, perhaps with a couple keypoints marked. And I wanted it to be dead-simple, so that with a single function call and a few seconds I could see what sort of image I was working with. So I wrote visualize_voxels.\nIt was my first time writing a Python library. Really, calling it a library is a stretch, because it only delivers to the user one function (visualize). But because I work in many environments (on my laptop, in online Jupyter notebooks, through SSH on BYU\u0026rsquo;s supercomputer), I wanted it to be easily installable via pip.\nThe result was exactly what I needed. Given a NumPy array of scalars arr, simply calling visualize(arr) produces a visualization like the one displayed below in seconds—quick, simple, effective. Then I started adding other useful features, like the ability to mark points in the volume, and change the size, speed, and resolution of the visualization, among other little things. I made it work seamlessly in both .py scripts as well as notebooks.\nThis code (which leverages tomogram-datasets as well) visualizes the tomogram with the third-largest number of flagellar motors in our supercomputer:\nfrom tomogram_datasets import all_fm_tomograms import numpy as np from visualize_voxels import visualize as viz tomos = all_fm_tomograms() n_flagellar_motors = [len(tomo.annotation_points()) for tomo in tomos] super_tomo = tomos[np.argsort(n_flagellar_motors)[-3]] viz( super_tomo.get_data(), marks=super_tomo.annotation_points(), markalpha=0.5, axis=0, slices=np.linspace(80, 320, 100), fps=16 ) Try playing with it here.\n","date":"13 November 2024","permalink":"/projects/visualize-voxels/","section":"Projects","summary":"","title":"visualize-voxels"},{"content":"I\u0026rsquo;m Matthew Ward, a student in the Applied and Computational Mathematics Emphasis (ACME) program at Brigham Young University, and a member of the BYU Biophysics Group.\nI focus on computer vision, optimization, and the mathematics behind data science and machine learning.\nMy wife and I When I\u0026rsquo;m not studying, I like to sing my wife songs at the piano. She and I play strings together in a non-music-major string orchestra at BYU. We love cooking and baking too.\n","date":null,"permalink":"/about/","section":"Home","summary":"","title":"About me"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"I would love to get in touch with you! Contact me here and I\u0026rsquo;ll get back to you soon.\nEmail Message Send Message\n","date":null,"permalink":"/contact/","section":"Home","summary":"","title":"Contact"},{"content":"I\u0026rsquo;m looking for an internship in the summer of 2025! Contact me via email at me@matthewward.info or through the Contact page on this site.\nThis resume page is interactive. Click on the green links to see relevant examples of my work.\nEducation #BS, Applied and Computational Mathematics Emphasis (ACME)\nBrigham Young University | Provo, Utah\nApril 2026\nConcentration: Data Science \u0026amp; Machine Learning GPA: 3.97 Academic Scholarship (3 years) Skills #Programming # Proficient in NumPy Pandas Python scikit-learn PyTorch Julia C\u0026#43;\u0026#43; SQL Java Unix Shell Experience with HTML/CSS VBA SystemVerilog Object-oriented programming Database management High-performance computing Git / GitHub Excel Data Science \u0026amp; Machine Learning # Data visualization Data analysis Digital image processing Computer vision Machine learning Deep learning Numerical and dynamic optimization Mathematics \u0026amp; Algorithms # Dynamic systems Numerical linear algebra Mathematical statistics Curriculum in 2025 #Bayesian modeling, hidden Markov models, state-space models, ARIMA models, optimal control, control theory\nExperience #Research Assistant\nBrigham Young University — Biophysics Simulation Group | Provo, Utah\nApril 2024–Present\nCollaborate with a multidisciplinary team of physicists, biochemists, mathematicians, and computer scientists to develop and evaluate particle picking, image segmentation, and template matching algorithms and pipelines in cryo- electron tomography Analyze, visualize and present statistical findings related to a vast database of over 50 terabytes of three-dimensional images Write and optimize Python and Julia code for advanced image processing and object recognition tasks using algorithms such as Canny edge detection, SLIC superpixels, U-Net, etc. Leverage supercomputer resources to run image processing tasks, improving computational efficiency and enabling the analysis of high-resolution data Maintain code integrity using Git and GitHub Financial and Executive Secretary\nThe Church of Jesus Christ of Latter-day Saints | Cali, Valle del Cauca, Colombia\nMarch 2022–February 2023\nStreamlined financial systems and managed travel plans for 150 full-time representatives Designed and developed a VBA-based software tool to categorize 10,000 poorly formatted addresses into geographically organized lists which updated dynamically based on location and user input Provided 24/7 logistical, technological, and financial support to the president of the organization Projects #See the Projects page on this site.\n","date":null,"permalink":"/resume/","section":"Home","summary":"","title":"Resume"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]