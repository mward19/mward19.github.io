[{"content":"This project is a paper with associated code.\nAs a student in BYU\u0026rsquo;s Applied and Computational Mathematics Emphasis (ACME), this semester I took a class in modeling with data and uncertainty. In it my peers and I learned (among many other things) about the theoretical underpinnings of various classification and regression techniques. To enhance our practical understanding of machine learning, we were assigned an open-ended group project—given a list of a dozen or so datasets from Kaggle, we were pick a dataset, ask some interesting question about the data, and attempt to answer it with the techniques we\u0026rsquo;d learned this semester. Then we were to summarize our findings in a five-page paper.\nI wrote the paper (paper, associated code) with my classmate Rebecca Gee. She focused primarily on data preparation and understanding what the features in the dataset actually meant (what the heck is \u0026ldquo;Income Composition of Resources\u0026rdquo;? She figured it out), and I focused on the feature engineering and machine learning methods.\nI usually manage my code with Git and GitHub, but this time I opted for a simple Jupyter notebook in Google Colab.\n","date":"12 December 2024","permalink":"/projects/acme-volume-3-project/","section":"Projects","summary":"","title":"Predicting future life expectancy in countries using present data"},{"content":"","date":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":" As a student in applied mathematics, I spend much of my time writing code, taking lecture notes, and working out proofs, all at breakneck speed. Many of my peers use \\(\\LaTeX\\) to take notes and do homework. While I appreciate \\(\\LaTeX\\) and often use it for formal write-ups, I don\u0026rsquo;t love not being able to quickly draw diagrams, change colors, make up new notation on the fly, etc. Handwriting is infinitely flexible, and in my experience, much quicker when I\u0026rsquo;m trying to pump out my homework and move on.\nA notebook isn\u0026rsquo;t enough. I want my notes to be stored in the cloud. I want them to be organized and easily accessible. I want them to be stored in .pdf format so they\u0026rsquo;re future-proof. And I don\u0026rsquo;t want to buy a tablet. Why? Well, good tablets are expensive, I like my laptop, I don\u0026rsquo;t want to bring two devices to school, and I like being able to seamlessly copy snippets of code I\u0026rsquo;m writing and textbooks I\u0026rsquo;m referencing into my notes, among other things.\nAbout two years ago, the solution came to me. I had the idea when I was a missionary in Cali, Colombia, walking down the street just a few months before it was time for me to go home. I was an office secretary and I had just used the mission credit card to pay at a notary, and signed for it using a screenless credit card signature pad attached to the card reader.\nI thought as I walked, \u0026ldquo;why don\u0026rsquo;t they just take one of those and scale it up?\u0026rdquo; I was certain that such a product must exist.\nSure enough, it does—but it isn\u0026rsquo;t generally marketed to people like me. Many companies, the foremost of which is Wacom, manufacture screenless tablets with pressure-sensitive pens to do digital art and animation. The level of quality varies greatly. The fanciest one I\u0026rsquo;ve used has a screen and sells for $3,500 USD (The research group I currently work with has one to annotate tomograms—it\u0026rsquo;s a Wacom Cintiq Pro 27).\nI don\u0026rsquo;t have that kind of cash. So I saved a few hundred bucks and got one of these on Amazon for $50.\nThere\u0026rsquo;s really nothing special about this specific model (Huion H610PRO v2). It\u0026rsquo;s just a generic off-brand screenless Wacom tablet that connects to your computer through a USB cable. People usually use these to do digital art and animation. I have found that they work astoundingly well for notetaking as well. Neither the pen nor the tablet use batteries, so when I\u0026rsquo;m on the go, all I have to charge is my laptop.\nI\u0026rsquo;ve been using my tablet for a year and a half now, and it\u0026rsquo;s still working like new. I only had to replace the pen nib once because I was fiddling with it and lost it on the floor. Whoops!\nThe trickiest part about all this is how to set it up. Naturally with unstandardized hardware like a screenless tablet there are lots of driver issues and subtleties to keep in mind. In addition, I use Linux (Debian 12 with GNOME and KDE), so things are often a little less intuitive (but more fun). Huion (the manufacturer) makes a good driver, and I also like using OpenTabletDriver, an open-source alternative. Things get a little dicey with a multi-monitor setup but I think I\u0026rsquo;ve got it figured out now.\nI take notes in Xournal++. I love it. It\u0026rsquo;s everything I needed in notetaking software and more. My tablet has eight buttons on the left, which I\u0026rsquo;ve mapped to the following in Xournal++: undo, delete, pencil, zoom in, zoom out, new page, the lasso selection tool, and the hand tool. I also have an on-screen icon in my GNOME panel to take screenshots (Screenshot Tool), which is especially useful when I want to paste snippets from textbooks (all of which I access digitally) into my homework. As a result, my workflow is very fast!\nI deliberately chose to map the buttons to shortcuts for which I wouldn\u0026rsquo;t want to move my pen. For example, if I\u0026rsquo;m writing a word and I mess up, I can undo my mistake without having to change my pen position, so I can rewrite it immediately. I can select and move a group of objects with the lasso without having to move away. I have come to realize that it is much faster than a traditional tablet because of these shortcut keys. Maybe an iPad would be better with some kind of shortcut keypad\u0026hellip; looks like that exists too.\nConveniently, the tablet has little rubber feet that fit perfectly over my laptop keyboard and trackpad. I use a 15.6 inch Lenovo ThinkPad, and by complete coincidence, the Huion tablet is exactly the same size. It\u0026rsquo;s perfect for when I\u0026rsquo;m taking notes in a lecture hall or the ACME study room is too full for me to be able to really spread out.\nWhen I do have room to spread out, I take advantage of the space.\nPeople ask me about my tablet all the time—but usually it\u0026rsquo;s after they\u0026rsquo;ve noticed my mouse. Like most people, I used to use a normal one. I\u0026rsquo;m relatively tall, so I have big hands. Something about gripping a tiny plastic mouse, scrunching up my hand, and moving it around for hours like that made my hand and wrist start to hurt. It would be a real bummer to get carpal tunnel syndrome, so I took action before it was too late and started looking into alternatives to the usual mouse.\nThat\u0026rsquo;s when I discovered trackballs. What a revelation! I picked up a Kensington Orbit a year and a half ago (a couple months after getting the tablet, I think) and have never looked back. My wrist and hand have never hurt since.\nBecause it\u0026rsquo;s larger and wider than a normal mouse, it opens my hand up. That\u0026rsquo;s the real key, I think. It also takes up a lot less space because it stays in place unlike a mouse, which is a huge plus when I\u0026rsquo;m in a crowded study room or computer lab. I love the tactile feel of moving the ball around and scrolling with the ring, it feels more natural to me. It took time to get used to it, but these days I bring the thing everywhere.\nThis setup has been a game-changer for me as a student. It’s affordable, reliable, and works well with my workflow. Sometimes the best solutions are the ones you figure out by thinking a little outside the box. If nothing else, it’s been fun to experiment and find what works best for me.\n","date":"25 November 2024","permalink":"/posts/my-weird-workflow/","section":"Posts","summary":"","title":"A cheaper alternative to tablets, and the mouse to rule them all"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"This project is hosted on GitHub.\nIn the BYU Biophysics research group, we spend much of our time working with 3-dimensional images of bacteria called tomograms. Researchers have spent a lot of time looking for structures in these tomograms, and save their findings in annotation files. tomogram_datasets makes it easier to navigate the web of tomograms and annotations we have, simplifying analysis and dataset creation. While it\u0026rsquo;s something I\u0026rsquo;m still working on, I use it every day in my research.\nThe code is hosted on GitHub.\nIt puts tomograms and their respective annotations into an object-oriented framework, so that accessing attributes of a particular tomogram, like annotations, supercomputer filepath, or header data, is quick and easy. This is primarily to facilitate the creation and analysis of competition datasets. Our group has already done a couple Kaggle competitions internally at BYU, and as we prepare to launch our first worldwide competition, I found myself in need of an easier way to work with our tomograms.\nThe project was also an opportunity to practice robust coding practices (implementing unit tests, thorough documentation), scripting (file management on a supercomputer, file loading), as well as data processing.\nIn addition, I learned a lot about Python libraries making this, like how to define the dependencies of my project so users could install it without having to think about that. I think my favorite part was learning to generate a documentation site automatically from the code\u0026rsquo;s docstrings using MkDocs. Seeing it update itself as I added features was fascinating.\n","date":"13 November 2024","permalink":"/projects/tomogram-datasets/","section":"Projects","summary":"","title":"tomogram-datasets"},{"content":" This project is hosted on GitHub.\nBrief summary #While looking for ways to work with and extract information from cryo-ET tomograms (noisy 3D images) in the summer of 2024, I read a book about ways of making the most of messy data using linear algebra and optimization. In the process, I learned about Robust Principal Component Analysis (RPCA), which can be performed with Principal Component Pursuit (PCP).\nHere I apply Principal Component Pursuit to a video I shot in my lab. Using this method, I am able to extract the background and foreground of a video, using nothing more than linear algebra and a simple convex optimization problem.\nThe top video is the original, which I shot of myself in my lab. The middle video is the \u0026ldquo;background\u0026rdquo; (not moving) component of the video. The bottom video is the \u0026ldquo;foreground\u0026rdquo; (moving) component of the video. This project is inspired by a common application of PCP: identifying video segments in surveillance camera feeds where something of interest is happening. The video above simulates this in just a few seconds.\nI have yet to successfully apply this method to tomograms, but my advisor and I thought it was fascinating, so we feel like we succeeded anyway! I implemented the method in Julia, a language I am coming to love. See my implementation code on GitHub.\nIf you would like to learn more details, read the longer summary below.\nLonger summary #What is this? #This is an implementation and demonstration of Principal Component Pursuit, a way to solve the Robust Principal Component Analysis problem, which is here applied to a short video I shot.\nWhat am I seeing? #I recorded the top video in my lab. I wanted a video with a mostly static background and something moving in the foreground.\nUsing the method, which I will describe next, I separate the video into a static component and a moving component. The static component is the middle video. The moving component is the bottom video. In other words, the bottom video added to the middle video yields the top video.\nHow does it work? #Each frame of a grayscale video can be thought of as a matrix of grayscale values. For each frame, I take that matrix and flatten it into a vector. Thus, each frame of the video can be represented as a long vector with as many elements as there are pixels in a frame. I will call this vector a \u0026ldquo;frame vector\u0026rdquo;.\nBy representing the frames as vectors, the entire video can itself be represented as a matrix. This is done by stacking all of the frame vectors side by side into a huge matrix, with as many columns as there are frames in the video. I will call this matrix a \u0026ldquo;video matrix\u0026rdquo;.\nIn a video with a mostly static background and something moving in the foreground, the video matrix is almost low rank, since the frame vectors are mostly the same (since most of the pixels don\u0026rsquo;t change). But it isn\u0026rsquo;t, because of the movement in the foreground. Nevertheless, we can find a video matrix that is nearly equal to the original video matrix but is in fact low rank. In simpler terms, we can extract the background of the video.\nLet \\(\\bf{Y}\\) be the original video matrix. We want a video matrix \\(\\bf{L}\\) that is nearly equal to \\(\\bf{Y}\\), differing only by a sparse (meaning most of the elements are 0) matrix \\(\\bf{L}\\). In other words, we want to find \\(\\bf{L}\\) and \\(\\bf{S}\\) such that \\(\\bf{Y} = \\bf{L} + \\bf{S}\\), while minimizing \\(\\text{rank}(\\bf{L})\\) and \\(\\Vert\\bf{S}\\Vert_0\\) (where \\(\\Vert\\cdot\\Vert_0\\) gives the number of non-zero elements of its input, which technically is not a proper mathematical norm).\nOne could set this up as an optimization problem\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\text{rank}(\\bf{L}) + \\lambda \\Vert\\bf{S}\\Vert_0 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S} \\end{aligned} $$\nfor some tuning parameter \\(\\lambda \\in \\mathbb{R}\\), but the objective is not convex, making this very difficult to solve.\nRather, we set up the problem using convex surrogate norms:\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\Vert\\bf{L}\\Vert_* + \\lambda \\Vert\\bf{S}\\Vert_1 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S}, \\end{aligned} $$\nwhere \\(\\Vert \\cdot \\Vert_*\\) is the nuclear norm, meaning, the sum of the singular values of the input matrix, and \\(\\Vert \\cdot \\Vert_1\\) is the standard matrix 1-norm (the maximum column sum). (See \u0026ldquo;Note: Why these norms?\u0026rdquo; below.)\nThis new problem is convex! It is easy to solve with off-the-shelf convex optimizers. I have opted to implement the optimizer myself, but other libraries like CVXPY (in Python) or Convex.jl (for Julia) should work fine.\nBy solving\n$$ \\begin{aligned} \\text{minimize}\\quad\u0026amp;\\Vert\\bf{L}\\Vert_* + \\lambda \\Vert\\bf{S}\\Vert_1 \\\\ \\text{subject to}\\quad\u0026amp;\\bf{Y} = \\bf{L} + \\bf{S}, \\end{aligned} $$\nwe find video matrices \\(\\bf{L}\\) and \\(\\bf{S}\\) that, for all intents and purposes, separate the original video matrix \\(\\bf{Y}\\) into \u0026ldquo;background\u0026rdquo; and \u0026ldquo;foreground\u0026rdquo; components respectively. Problem solved!\nNote: Why these norms? #First, we want to minimize the rank of \\(\\bf{L}\\). When the rank of \\(\\bf{L}\\) is minimized, we hope that most of its singular values are zero. Perhaps this will shed some intuition on why the nuclear norm makes sense here.\nSecond, we want to minimize the number of nonzero elements of \\(\\bf{S}\\) (what I called \\(\\Vert \\cdot \\Vert_0\\) above). When the number of nonzero elements of \\(\\bf{S}\\) is minimized, we would hope that the maximum column sum of \\(\\bf{S}\\) is quite small. Hopefully this clarifies why the 1-norm is a reasonable choice.\nFor more formal justification for these choices of norm, consult Wright and Ma\u0026rsquo;s textbook High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications.\n","date":"13 November 2024","permalink":"/projects/principal-component-pursuit/","section":"Projects","summary":"","title":"Principal Component Pursuit"},{"content":"Applied Mathematics student at Brigham Young University\n","date":null,"permalink":"/","section":"Home","summary":"","title":"Home"},{"content":"This project is hosted on GitHub.\nA helix plotted within a tomogram. I spend a lot of time working with cryo-electron tomograms. They\u0026rsquo;re huge, noisy, three-dimensional images that can take up gigabytes of storage apiece.\nNaturally, visualizing these images is a pain—most of the options are desktop applications like IMOD. napari is marvelous, but heavier than I usually need. All the time I found myself in a Jupyter notebook working doing data analysis on tomograms and I just wanted a quick snapshot of what the volume looks like, perhaps with a couple keypoints marked. And I wanted it to be dead-simple, so that with a single function call and a few seconds I could see what sort of image I was working with. So I wrote visualize_voxels.\nIt was my first time writing a Python library. Really, calling it a library is a stretch, because it only delivers to the user one function (visualize). But because I work in many environments (on my laptop, in online Jupyter notebooks, through SSH on BYU\u0026rsquo;s supercomputer), I wanted it to be easily installable via pip.\nThe result was exactly what I needed. Given a NumPy array of scalars arr, simply calling visualize(arr) produces a visualization like the one displayed below in seconds—quick, simple, effective. Then I started adding other useful features, like the ability to mark points in the volume, and change the size, speed, and resolution of the visualization, among other little things. I made it work seamlessly in both .py scripts as well as notebooks.\nThis code (which leverages tomogram-datasets as well) visualizes the tomogram with the third-largest number of flagellar motors in our supercomputer:\nfrom tomogram_datasets import all_fm_tomograms import numpy as np from visualize_voxels import visualize as viz tomos = all_fm_tomograms() n_flagellar_motors = [len(tomo.annotation_points()) for tomo in tomos] super_tomo = tomos[np.argsort(n_flagellar_motors)[-3]] viz( super_tomo.get_data(), marks=super_tomo.annotation_points(), markalpha=0.5, axis=0, slices=np.linspace(80, 320, 100), fps=16 ) Try playing with it here.\n","date":"13 November 2024","permalink":"/projects/visualize-voxels/","section":"Projects","summary":"","title":"visualize-voxels"},{"content":"I\u0026rsquo;m Matthew Ward, a student in the Applied and Computational Mathematics Emphasis (ACME) program at Brigham Young University, and a member of the BYU Biophysics Group.\nI am especially interested in computer vision, optimization, and the mathematics behind data science and machine learning.\nWhen I\u0026rsquo;m not studying, I like to sing my wife songs at the piano. She and I play strings together in a non-music-major string orchestra at BYU. We love cooking and baking too.\n","date":null,"permalink":"/about/","section":"Home","summary":"","title":"About me"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"I would love to get in touch with you! Contact me here and I\u0026rsquo;ll get back to you soon.\nEmail Message Send Message\n","date":null,"permalink":"/contact/","section":"Home","summary":"","title":"Contact"},{"content":" Qwixx This website PCP ","date":"1 January 0001","permalink":"/projects/todo/","section":"Projects","summary":"","title":"Projects to write about"},{"content":" Education #BS, Applied and Computational Mathematics Emphasis (ACME)\nBrigham Young University | Provo, Utah\nApril 2026\nConcentration: Data Science \u0026amp; Machine Learning GPA: 3.98 Academic Scholarship (3 years) Skills #Programming # Proficient in NumPy Pandas Python scikit-learn PyTorch Julia C\u0026#43;\u0026#43; SQL Java Unix Shell Experience with HTML/CSS VBA SystemVerilog Object-oriented programming Database management High-performance computing Git / GitHub Excel Data Science \u0026amp; Machine Learning # Data visualization Data analysis Digital image processing Computer vision Machine learning Deep learning Numerical and dynamic optimization Mathematics \u0026amp; Algorithms # Dynamic systems Numerical linear algebra Mathematical statistics Curriculum in 2025 #Bayesian modeling, hidden Markov models, state-space models, ARIMA models, optimal control, control theory\nExperience #Research Assistant\nBrigham Young University — Biophysics Simulation Group | Provo, Utah\nApril 2024–Present\nCollaborate with a multidisciplinary team of physicists, biochemists, mathematicians, and computer scientists to develop and evaluate particle picking, image segmentation, and template matching algorithms and pipelines in cryo- electron tomography Analyze, visualize and present statistical findings related to a vast database of over 50 terabytes of three-dimensional images Write and optimize Python and Julia code for advanced image processing and object recognition tasks using algorithms such as Canny edge detection, SLIC superpixels, U-Net, etc. Leverage supercomputer resources to run image processing tasks, improving computational efficiency and enabling the analysis of high-resolution data Maintain code integrity using Git and GitHub Financial and Executive Secretary\nThe Church of Jesus Christ of Latter-day Saints | Cali, Valle del Cauca, Colombia\nMarch 2022–February 2023\nStreamlined financial systems and managed travel plans for 150 full-time representatives Designed and developed a VBA-based software tool to categorize 10,000 poorly formatted addresses into geographically organized lists which updated dynamically based on location and user input Provided 24/7 logistical, technological, and financial support to the president of the organization Projects #See the Projects page on this site.\n","date":null,"permalink":"/resume/","section":"Home","summary":"","title":"Resume"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]